# üìã –†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –º–∏–≥—Ä–∞—Ü–∏–∏ –Ω–∞ v2.0

–≠—Ç–æ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ–º–æ–∂–µ—Ç –≤–∞–º –º–∏–≥—Ä–∏—Ä–æ–≤–∞—Ç—å —Å –ø—Ä–µ–¥—ã–¥—É—â–µ–π –≤–µ—Ä—Å–∏–∏ –Ω–∞ –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É ML Data Pipeline v2.0.

## üéØ –ß—Ç–æ –∏–∑–º–µ–Ω–∏–ª–æ—Å—å

### –û—Å–Ω–æ–≤–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è

1. **–ú–æ–¥—É–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞** - –∫–æ–¥ —Ä–∞–∑–¥–µ–ª–µ–Ω –Ω–∞ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –≤ `src/pipeline/`
2. **PydanticAI** - AI-–∞–≥–µ–Ω—Ç—ã —Ç–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É—é—Ç PydanticAI –¥–ª—è —Ç–∏–ø–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
3. **Pydantic Settings** - –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ `src/config_v2.py`
4. **–í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤** - –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –≤–µ—Ä—Å–∏–π —Å git-like –∫–æ–º–∞–Ω–¥–∞–º–∏
5. **HITL —É–ª—É—á—à–µ–Ω–∏—è** - —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π ReviewDataset —Å –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–µ–π

### –û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å

‚úÖ **–í—Å–µ —Å—Ç–∞—Ä—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –æ—Å—Ç–∞—é—Ç—Å—è —Ä–∞–±–æ—Ç–∞—é—â–∏–º–∏** —á–µ—Ä–µ–∑ –æ–±—ë—Ä—Ç–∫–∏ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏.

---

## üöÄ –ü–æ—ç—Ç–∞–ø–Ω–∞—è –º–∏–≥—Ä–∞—Ü–∏—è

### –§–∞–∑–∞ 1: –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π (5 –º–∏–Ω)

1. **–û–±–Ω–æ–≤–∏—Ç–µ requirements.txt:**

```bash
pip install -r requirements.txt
```

–ù–æ–≤—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:
- `pydantic-ai>=0.0.14` - –¥–ª—è AI-–∞–≥–µ–Ω—Ç–æ–≤
- `pydantic-settings>=2.5` - –¥–ª—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
- `ollama>=0.1.0` - –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π

2. **–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —É—Å—Ç–∞–Ω–æ–≤–∫—É:**

```bash
python -c "import pydantic_ai; print('PydanticAI installed:', pydantic_ai.__version__)"
```

---

### –§–∞–∑–∞ 2: –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (10 –º–∏–Ω)

#### –°—Ç–∞—Ä–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (`src/config.py`):

```python
from src.config import Settings
settings = Settings.load()

print(settings.llm_api_key)
print(settings.llm_model)
print(settings.batch_size)
```

#### –ù–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (`src/config_v2.py`):

```python
from src.config_v2 import Settings
settings = Settings.load()

# –î–æ—Å—Ç—É–ø –∫ –∫–æ–Ω—Ñ–∏–≥–∞–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
print(settings.telegram.bot_token)
print(settings.llm.model)
print(settings.labeler.batch_size)
print(settings.augmenter.variants_per_sample)
```

#### –ú–∏–≥—Ä–∞—Ü–∏—è .env —Ñ–∞–π–ª–∞:

–°—Ç–∞—Ä—ã–π `.env`:
```env
TELEGRAM_BOT_TOKEN=...
LLM_API_KEY=...
LLM_MODEL=gpt-4o-mini
BATCH_SIZE=20
LOW_CONF=0.5
```

–ù–æ–≤—ã–π `.env` (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π):
```env
# Telegram
TELEGRAM_BOT_TOKEN=...

# LLM
LLM_API_KEY=...
LLM_MODEL=gpt-4o-mini

# Labeler
LABELER_BATCH_SIZE=20
LABELER_LOW_CONF_THRESHOLD=0.5

# Augmenter
AUGMENTER_VARIANTS_PER_SAMPLE=3
AUGMENTER_CONCURRENCY=8

# Review (HITL)
REVIEW_LOW_CONFIDENCE_THRESHOLD=0.5

# Data Writer
DATA_WRITER_EVAL_FRACTION=0.1
DATA_WRITER_BALANCE_DOMAINS=true

# Data Storage
DATA_STORAGE_MAX_VERSIONS=100

# Cache
CACHE_TTL_HOURS=24

# App
APP_MODE=development
APP_LOG_LEVEL=INFO
```

**üí° Tip:** –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ `config.example.v2` –∫–∞–∫ —à–∞–±–ª–æ–Ω.

---

### –§–∞–∑–∞ 3: –ú–∏–≥—Ä–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

–í—ã –º–æ–∂–µ—Ç–µ –º–∏–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ –∏–ª–∏ —Å—Ä–∞–∑—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–æ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã.

#### 3.1 ETL Component

**–°—Ç–∞—Ä—ã–π –∫–æ–¥:**
```python
from src.etl import normalize_file_to_df

df = normalize_file_to_df(Path("data/logs.xlsx"), max_rows=10000)
```

**–ù–æ–≤—ã–π –∫–æ–¥:**
```python
from src.pipeline.etl import ETLProcessor, ETLConfig

config = ETLConfig(max_rows=10000, deduplicate=True)
processor = ETLProcessor(config)
df = processor.process_file(Path("data/logs.xlsx"))

# –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
stats = processor.get_stats()
print(f"Processed: {stats['processed_rows']}, Filtered: {stats['filtered_rows']}")
```

**–°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å:** –°—Ç–∞—Ä–∞—è —Ñ—É–Ω–∫—Ü–∏—è `normalize_file_to_df` –≤—Å—ë –µ—â—ë —Ä–∞–±–æ—Ç–∞–µ—Ç!

---

#### 3.2 Labeler Agent

**–°—Ç–∞—Ä—ã–π –∫–æ–¥:**
```python
from src.labeler import label_dataframe_batched, classify_one
from src.llm import LLMClient

client = LLMClient(api_key="...", api_base=None, model="gpt-4o-mini")

# –û–¥–∏–Ω —Ç–µ–∫—Å—Ç
result = classify_one(client, system_prompt, fewshot, "–ø–µ—Ä–µ–¥–∞—Ç—å –ø–æ–∫–∞–∑–∞–Ω–∏—è")

# Batch
results = await label_dataframe_batched(
    df, client, system_prompt, fewshot, 
    batch_size=20, rate_limit=0.4
)
```

**–ù–æ–≤—ã–π –∫–æ–¥:**
```python
from src.pipeline.labeler_agent import LabelerAgent, LabelerConfig

config = LabelerConfig(
    model="gpt-4o-mini",
    api_key="...",
    batch_size=20,
    rate_limit=0.4,
    use_cache=True
)

agent = LabelerAgent(config)

# –û–¥–∏–Ω —Ç–µ–∫—Å—Ç - —Ç–∏–ø–æ–±–µ–∑–æ–ø–∞—Å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç!
result: ClassificationResult = await agent.classify_one("–ø–µ—Ä–µ–¥–∞—Ç—å –ø–æ–∫–∞–∑–∞–Ω–∏—è")
print(f"Domain: {result.domain_id}, Confidence: {result.confidence}")

# Batch
results: List[ClassificationResult] = await agent.classify_dataframe(df)

# –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
low_conf = agent.get_low_confidence_items(results, threshold=0.5)

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
stats = agent.get_stats()
print(f"Cache hits: {stats['cache_hits']}, LLM calls: {stats['llm_calls']}")
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –Ω–æ–≤–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞:**
- ‚úÖ –¢–∏–ø–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å —á–µ—Ä–µ–∑ Pydantic
- ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è
- ‚úÖ –í—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
- ‚úÖ –õ—É—á—à–µ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ

**–°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å:** –°—Ç–∞—Ä—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ —Ä–∞–±–æ—Ç–∞—é—Ç —á–µ—Ä–µ–∑ –Ω–æ–≤—ã–π –∞–≥–µ–Ω—Ç –ø–æ–¥ –∫–∞–ø–æ—Ç–æ–º!

---

#### 3.3 Augmenter Agent

**–°—Ç–∞—Ä—ã–π –∫–æ–¥:**
```python
from src.augmenter import augment_dataset

synthetic = await augment_dataset(
    llm_client, system_prompt, items,
    rate_limit=0.1, 
    include_low_conf=False,
    only_positive=True,
    concurrency=8
)
```

**–ù–æ–≤—ã–π –∫–æ–¥:**
```python
from src.pipeline.augmenter_agent import AugmenterAgent, AugmenterConfig

config = AugmenterConfig(
    model="gpt-4o-mini",
    api_key="...",
    variants_per_sample=3,
    include_hard_negatives=False,
    concurrency=8,
    rate_limit=0.1
)

agent = AugmenterAgent(config)

# –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π
synthetic = await agent.augment_batch(items, balance_domains=True)

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
stats = agent.get_stats()
print(f"Generated {stats['total_generated']} samples, Cache hits: {stats['cache_hits']}")
```

---

#### 3.4 Review Dataset (HITL)

**–°—Ç–∞—Ä—ã–π –∫–æ–¥:**
```python
from src.store import Store

store = Store(Path("data"))

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –æ—á–µ—Ä–µ–¥—å
store.append_hitl_queue(low_conf_items)

# –ß—Ç–µ–Ω–∏–µ –æ—á–µ—Ä–µ–¥–∏
items = store.read_hitl_queue(limit=10)
```

**–ù–æ–≤—ã–π –∫–æ–¥:**
```python
from src.pipeline.review_dataset import ReviewDataset, ReviewDatasetConfig

config = ReviewDatasetConfig(
    data_dir=Path("data"),
    low_confidence_threshold=0.5,
    high_priority_threshold=0.3
)

review = ReviewDataset(config)

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–µ–π
review.add_items(low_conf_items)

# –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ
items = review.get_next(count=1, reviewer_id="user123")

# –û—Ç–ø—Ä–∞–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
review.submit_review(
    item_id=items[0].id,
    corrected_domain="house",
    reviewer_id="user123",
    notes="–û—á–µ–≤–∏–¥–Ω–æ –ñ–ö–•"
)

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—á–µ—Ä–µ–¥–∏
stats = review.get_queue_stats()
print(f"Queue: {stats['queue_size']}, By priority: {stats['by_priority']}")

# –≠–∫—Å–ø–æ—Ä—Ç –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö
reviewed_path = review.export_reviewed()
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- ‚úÖ –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
- ‚úÖ –°—Ç–∞—Ç—É—Å—ã —ç–ª–µ–º–µ–Ω—Ç–æ–≤ (PENDING, IN_REVIEW, APPROVED, CORRECTED)
- ‚úÖ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
- ‚úÖ –≠–∫—Å–ø–æ—Ä—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è

---

#### 3.5 DataWriter - NEW! üÜï

**–°—Ç–∞—Ä—ã–π –∫–æ–¥:**
```python
from src.augmenter import split_train_eval, write_jsonl

# –°–ø–ª–∏—Ç
train, eval_ = split_train_eval(items, eval_frac=0.1, min_eval=50)

# –ó–∞–ø–∏—Å—å
write_jsonl(Path("data/artifacts/dataset_train.jsonl"), train)
write_jsonl(Path("data/artifacts/dataset_eval.jsonl"), eval_)
```

**–ù–æ–≤—ã–π –∫–æ–¥:**
```python
from src.pipeline.data_writer import DataWriter, DataWriterConfig

config = DataWriterConfig(
    output_dir=Path("data/artifacts"),
    eval_fraction=0.1,
    min_eval_samples=50,
    balance_domains=True,
    validate_quality=True,
    include_metadata=True
)

writer = DataWriter(config)

# –ó–∞–ø–∏—Å—å —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
train_path, eval_path, stats = writer.write_datasets(
    items,
    dataset_name="production"
)

print(f"Train: {stats.train_samples}, Eval: {stats.eval_samples}")
print(f"Domains: {stats.domain_distribution}")
print(f"Avg confidence: {stats.avg_confidence:.2f}")
print(f"Quality issues: {stats.quality_issues}")
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- ‚úÖ –°—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–ª–∏—Ç
- ‚úÖ –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –¥–æ–º–µ–Ω–æ–≤
- ‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞
- ‚úÖ –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
- ‚úÖ –®–∞—Ä–¥–∏–Ω–≥ –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤

---

#### 3.6 DataStorage - NEW! üÜï

–ü–æ–ª–Ω–æ—Å—Ç—å—é –Ω–æ–≤—ã–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –¥–ª—è –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤.

```python
from src.pipeline.data_storage import DataStorage, DataStorageConfig, VersionStatus

config = DataStorageConfig(
    storage_dir=Path("data/storage"),
    max_versions=100,
    auto_archive_old=True
)

storage = DataStorage(config)

# –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ—Ä—Å–∏–∏
version = storage.commit_version(
    train_path=train_path,
    eval_path=eval_path,
    version_tag="v1.2.0",  # –∏–ª–∏ None –¥–ª—è auto-increment
    description="Added feedback, balanced domains",
    status=VersionStatus.STABLE,
    created_by="bot_pipeline"
)

# –¢–µ–≥–∏
storage.tag_version("v1.2.0", "production")
storage.tag_version("v1.2.0", "latest")

# –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –≤–µ—Ä—Å–∏–∏
storage.checkout("v1.2.0")

# –°–ø–∏—Å–æ–∫ –≤–µ—Ä—Å–∏–π
versions = storage.list_versions(status=VersionStatus.STABLE)

# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ
diff = storage.compare_versions("v1.1.0", "v1.2.0")

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
stats = storage.get_stats()
print(f"Total versions: {stats['total_versions']}")
print(f"Current: {stats['current_version']}")
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- ‚úÖ Git-like –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
- ‚úÖ –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ–≥–∏ (v1.2.3)
- ‚úÖ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤–µ—Ä—Å–∏–π
- ‚úÖ –ê–≤—Ç–æ–∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–∏–µ
- ‚úÖ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞

---

### –§–∞–∑–∞ 4: –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–æ—Ç–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

Telegram –±–æ—Ç (`src/bot.py`) —É–∂–µ –æ–±–Ω–æ–≤–ª–µ–Ω –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –Ω–æ–≤—ã–º–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏ —á–µ—Ä–µ–∑ —Ñ—É–Ω–∫—Ü–∏–∏ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏.

**–ß—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏:**
- ‚úÖ –í—Å–µ –∫–æ–º–∞–Ω–¥—ã –±–æ—Ç–∞
- ‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤
- ‚úÖ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤
- ‚úÖ HITL –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
- ‚úÖ Feedback —Å–∏—Å—Ç–µ–º–∞

**–ß—Ç–æ –º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å:**

1. **–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–æ–≤—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é:**

```python
# –°—Ç–∞—Ä–æ–µ
from src.config import Settings
settings = Settings.load()

# –ù–æ–≤–æ–µ
from src.config_v2 import Settings
settings = Settings.load()
```

2. **–í–∫–ª—é—á–∏—Ç—å –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤:**

```python
# –ü–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è dataset_train.jsonl –∏ dataset_eval.jsonl
from src.pipeline.data_storage import DataStorage, DataStorageConfig

storage = DataStorage(DataStorageConfig(storage_dir=Path("data/storage")))
version = storage.commit_version(
    train_path=train_p,
    eval_path=eval_p,
    description="Bot upload processed",
    status=VersionStatus.DRAFT
)

await update.message.reply_text(f"‚úÖ Version created: {version.version_tag}")
```

---

## üîß –¢–∏–ø–∏—á–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –º–∏–≥—Ä–∞—Ü–∏–∏

### –°—Ü–µ–Ω–∞—Ä–∏–π 1: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –º–∏–≥—Ä–∞—Ü–∏—è (—Ä–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ —Ä–∞–Ω—å—à–µ)

```bash
# 1. –û–±–Ω–æ–≤–ª—è–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
pip install -r requirements.txt

# 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º
python health_check.py

# 3. –ó–∞–ø—É—Å–∫–∞–µ–º –±–æ—Ç–∞ (–≤—Å—ë —Ä–∞–±–æ—Ç–∞–µ—Ç!)
python -m src.bot
```

‚úÖ **–ì–æ—Ç–æ–≤–æ!** –°–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –Ω–æ–≤—ã–º–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏ —á–µ—Ä–µ–∑ —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ API.

---

### –°—Ü–µ–Ω–∞—Ä–∏–π 2: –ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–∞—è –º–∏–≥—Ä–∞—Ü–∏—è (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)

**–ù–µ–¥–µ–ª—è 1: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è**
```python
# –û–±–Ω–æ–≤–ª—è–µ–º bot.py
from src.config_v2 import Settings
settings = Settings.load()
```

**–ù–µ–¥–µ–ª—è 2: ETL –∏ Labeler**
```python
# –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π ETL
from src.pipeline.etl import ETLProcessor
etl = ETLProcessor()

# –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π Labeler
from src.pipeline.labeler_agent import LabelerAgent
labeler = LabelerAgent(config)
```

**–ù–µ–¥–µ–ª—è 3: HITL –∏ DataWriter**
```python
# –û–±–Ω–æ–≤–ª—è–µ–º HITL
from src.pipeline.review_dataset import ReviewDataset
review = ReviewDataset(config)

# –ò—Å–ø–æ–ª—å–∑—É–µ–º DataWriter
from src.pipeline.data_writer import DataWriter
writer = DataWriter(config)
```

**–ù–µ–¥–µ–ª—è 4: –í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ**
```python
# –î–æ–±–∞–≤–ª—è–µ–º DataStorage
from src.pipeline.data_storage import DataStorage
storage = DataStorage(config)
```

---

### –°—Ü–µ–Ω–∞—Ä–∏–π 3: –ü–æ–ª–Ω–∞—è –º–∏–≥—Ä–∞—Ü–∏—è —Å—Ä–∞–∑—É

–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø–æ–ª–Ω—ã–π pipeline –∏–∑ –ø—Ä–∏–º–µ—Ä–∞ –≤ `README_V2.md`:

```python
from pathlib import Path
from src.pipeline import *
from src.config_v2 import Settings

async def full_pipeline():
    settings = Settings.load()
    
    # ETL
    etl = ETLProcessor()
    df = etl.process_file(Path("data/logs.xlsx"))
    
    # Labeler
    labeler = LabelerAgent.from_settings(settings)
    results = await labeler.classify_dataframe(df)
    
    # Review
    review = ReviewDataset.from_settings(settings)
    low_conf = labeler.get_low_confidence_items(results)
    review.add_items([r.dict() for r in low_conf])
    
    # Augmenter
    augmenter = AugmenterAgent.from_settings(settings)
    synthetic = await augmenter.augment_batch([r.dict() for r in results])
    
    # DataWriter
    writer = DataWriter.from_settings(settings)
    train_path, eval_path, stats = writer.write_datasets(
        [r.dict() for r in results] + [s.dict() for s in synthetic]
    )
    
    # DataStorage
    storage = DataStorage.from_settings(settings)
    version = storage.commit_version(train_path, eval_path)
    
    print(f"‚úÖ Pipeline completed! Version: {version.version_tag}")

import asyncio
asyncio.run(full_pipeline())
```

---

## üêõ –†–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º

### –ü—Ä–æ–±–ª–µ–º–∞: Import errors

```python
ModuleNotFoundError: No module named 'pydantic_ai'
```

**–†–µ—à–µ–Ω–∏–µ:**
```bash
pip install --upgrade -r requirements.txt
```

---

### –ü—Ä–æ–±–ª–µ–º–∞: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç

```python
ValidationError: TELEGRAM_BOT_TOKEN field required
```

**–†–µ—à–µ–Ω–∏–µ:**
```bash
# –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ .env —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
cp config.example.v2 .env

# –ó–∞–ø–æ–ª–Ω–∏—Ç–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
nano .env  # –∏–ª–∏ –ª—é–±–æ–π —Ä–µ–¥–∞–∫—Ç–æ—Ä
```

---

### –ü—Ä–æ–±–ª–µ–º–∞: –°—Ç–∞—Ä—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–µ —Ä–∞–±–æ—Ç–∞—é—Ç

```python
AttributeError: 'Settings' object has no attribute 'llm_model'
```

**–†–µ—à–µ–Ω–∏–µ:**
```python
# –í–º–µ—Å—Ç–æ —Å—Ç–∞—Ä–æ–≥–æ
settings.llm_model

# –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –Ω–æ–≤—ã–π
settings.llm.model
```

---

### –ü—Ä–æ–±–ª–µ–º–∞: –õ–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–µ —Ä–∞–±–æ—Ç–∞—é—Ç

```python
openai.OpenAIError: Connection error
```

**–†–µ—à–µ–Ω–∏–µ:**
```bash
# –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ —Å–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω
ollama serve

# –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å
curl http://localhost:11434/v1/models

# –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
LLM_API_BASE=http://localhost:11434/v1  # –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
LLM_MODEL=llama3.1:8b  # –º–æ–¥–µ–ª—å —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
```

---

## ‚úÖ –ß–µ–∫-–ª–∏—Å—Ç –º–∏–≥—Ä–∞—Ü–∏–∏

- [ ] –û–±–Ω–æ–≤–ª–µ–Ω—ã –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ (`pip install -r requirements.txt`)
- [ ] –°–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π `.env` —Ñ–∞–π–ª –∏–∑ `config.example.v2`
- [ ] –ó–∞–ø–æ–ª–Ω–µ–Ω—ã –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (TELEGRAM_BOT_TOKEN, LLM_API_KEY)
- [ ] –ü—Ä–æ–≤–µ—Ä–µ–Ω–∞ —Ä–∞–±–æ—Ç–∞ —á–µ—Ä–µ–∑ `health_check.py`
- [ ] –ó–∞–ø—É—â–µ–Ω –±–æ—Ç (`python -m src.bot`)
- [ ] –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∞ –∑–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞
- [ ] –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
- [ ] –ü—Ä–æ–≤–µ—Ä–µ–Ω–∞ —Ä–∞–±–æ—Ç–∞ HITL
- [ ] –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –æ–±–Ω–æ–≤–ª–µ–Ω –∫–æ–¥ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –Ω–æ–≤—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
- [ ] –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤

---

## üìö –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã

- [ARCHITECTURE_V2.md](ARCHITECTURE_V2.md) - –ø–æ–ª–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–∏—Å—Ç–µ–º—ã
- [README_V2.md](README_V2.md) - –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –∏ –ø—Ä–∏–º–µ—Ä—ã
- [config.example.v2](config.example.v2) - –ø—Ä–∏–º–µ—Ä –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏

---

## üí¨ –ü–æ–¥–¥–µ—Ä–∂–∫–∞

–ï—Å–ª–∏ —É –≤–∞—Å –≤–æ–∑–Ω–∏–∫–ª–∏ –ø—Ä–æ–±–ª–µ–º—ã —Å –º–∏–≥—Ä–∞—Ü–∏–µ–π:

1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ [Issues](https://github.com/your-username/esk-agent-llm-pro/issues)
2. –°–æ–∑–¥–∞–π—Ç–µ –Ω–æ–≤—ã–π Issue —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º –ø—Ä–æ–±–ª–µ–º—ã
3. –°–≤—è–∂–∏—Ç–µ—Å—å —Å —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞–º–∏

---

**–£–¥–∞—á–Ω–æ–π –º–∏–≥—Ä–∞—Ü–∏–∏! üöÄ**

