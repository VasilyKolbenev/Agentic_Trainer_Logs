# üöÄ ESK ML Data Pipeline v2.0

**Production-ready ML pipeline –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∏ –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LLM-–∞–≥–µ–Ω—Ç–æ–≤**

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Pydantic](https://img.shields.io/badge/pydantic-v2.8+-green.svg)](https://docs.pydantic.dev/)
[![PydanticAI](https://img.shields.io/badge/pydanticai-latest-purple.svg)](https://ai.pydantic.dev/)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)

## ‚ú® –ß—Ç–æ –Ω–æ–≤–æ–≥–æ –≤ v2.0

### üèóÔ∏è **–ú–æ–¥—É–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞**
- –ö–æ–º–ø–æ–Ω–µ–Ω—Ç–Ω—ã–π –¥–∏–∑–∞–π–Ω —Å —á–µ—Ç–∫–∏–º —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏
- 7 –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –º–æ–¥—É–ª–µ–π: ETL, Labeler, Augmenter, Review, DataWriter, DataStorage
- –õ–µ–≥–∫–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –∑–∞–º–µ–Ω–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤

### ü§ñ **PydanticAI –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è**
- –¢–∏–ø–æ–±–µ–∑–æ–ø–∞—Å–Ω—ã–µ AI-–∞–≥–µ–Ω—Ç—ã –¥–ª—è —Ä–∞–∑–º–µ—Ç–∫–∏ –∏ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —á–µ—Ä–µ–∑ Pydantic
- –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å LLM

### üåê **–õ–æ–∫–∞–ª—å–Ω—ã–µ LLM –º–æ–¥–µ–ª–∏**
- –ü–æ–ª–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ Ollama, vLLM, LM Studio
- –≠–∫–æ–Ω–æ–º–∏—è –Ω–∞ API calls
- –ö–æ–Ω—Ç—Ä–æ–ª—å –Ω–∞–¥ –¥–∞–Ω–Ω—ã–º–∏

### üì¶ **–í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤**
- Git-like –æ–ø–µ—Ä–∞—Ü–∏–∏ (commit, tag, checkout)
- –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ (v1.2.3)
- –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤–µ—Ä—Å–∏–π –∏ diff

### üîÑ **Human-in-the-Loop (HITL)**
- –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å feedback —Å–∏—Å—Ç–µ–º–æ–π
- –≠–∫—Å–ø–æ—Ä—Ç –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

### üìä **Production-ready**
- –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
- –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –º–µ—Ç—Ä–∏–∫–∏
- –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –∏ fallback
- –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞

---

## üìã –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ

- [–ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç](#-–±—ã—Å—Ç—Ä—ã–π-—Å—Ç–∞—Ä—Ç)
- [–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞](#-–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞)
- [–£—Å—Ç–∞–Ω–æ–≤–∫–∞](#-—É—Å—Ç–∞–Ω–æ–≤–∫–∞)
- [–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è](#-–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è)
- [–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ](#-–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ)
- [–õ–æ–∫–∞–ª—å–Ω—ã–µ LLM](#-–ª–æ–∫–∞–ª—å–Ω—ã–µ-llm)
- [API Reference](#-api-reference)
- [–ü—Ä–∏–º–µ—Ä—ã](#-–ø—Ä–∏–º–µ—Ä—ã)
- [–î–µ–ø–ª–æ–π](#-–¥–µ–ø–ª–æ–π)
- [Contributing](#-contributing)

---

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

### 1. –£—Å—Ç–∞–Ω–æ–≤–∫–∞

```bash
# –ö–ª–æ–Ω–∏—Ä—É–µ–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π
git clone https://github.com/your-username/esk-agent-llm-pro.git
cd esk-agent-llm-pro

# –°–æ–∑–¥–∞–µ–º –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ
python -m venv .venv

# –ê–∫—Ç–∏–≤–∏—Ä—É–µ–º (Windows)
.venv\Scripts\activate
# –ò–ª–∏ (Linux/Mac)
source .venv/bin/activate

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
pip install -r requirements.txt
```

### 2. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

```bash
# –ö–æ–ø–∏—Ä—É–µ–º –ø—Ä–∏–º–µ—Ä –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
cp config.example .env

# –†–µ–¥–∞–∫—Ç–∏—Ä—É–µ–º .env
# –ú–∏–Ω–∏–º—É–º –Ω—É–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å:
TELEGRAM_BOT_TOKEN=your_bot_token_here
LLM_API_KEY=your_openai_key_or_local_dummy
LLM_MODEL=gpt-4o-mini
```

### 3. –ó–∞–ø—É—Å–∫

```bash
# –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–∏—Å—Ç–µ–º—É
python health_check.py

# –ó–∞–ø—É—Å–∫–∞–µ–º –±–æ—Ç–∞
python -m src.bot
```

### 4. –ü–µ—Ä–≤—ã–π pipeline

```python
from pathlib import Path
from src.pipeline import ETLProcessor, LabelerAgent, DataWriter
from src.config_v2 import Settings

# –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
settings = Settings.load()

# ETL - –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞
etl = ETLProcessor()
df = etl.process_file(Path("data/logs.xlsx"))

# Labeler - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞
labeler = LabelerAgent.from_settings(settings)
results = await labeler.classify_dataframe(df)

# DataWriter - —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
writer = DataWriter.from_settings(settings)
train_path, eval_path, stats = writer.write_datasets(
    [r.dict() for r in results],
    dataset_name="my_dataset"
)

print(f"‚úÖ Created train ({stats.train_samples}) and eval ({stats.eval_samples})")
```

---

## üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ECK_Logs  ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   ETL   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇLabeler_Agent ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇReviewDataset ‚îÇ
‚îÇ(Telegram)  ‚îÇ    ‚îÇ(Process)‚îÇ    ‚îÇ(LLM-–∞–≥–µ–Ω—Ç)   ‚îÇ    ‚îÇ   (HITL)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                         ‚îÇ                     ‚îÇ
                                         ‚ñº                     ‚ñº
                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                              ‚îÇ Augmenter_Agent    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ          ‚îÇ
                              ‚îÇ (–°–∏–Ω—Ç–µ—Ç–∏–∫–∞)        ‚îÇ    ‚îÇ          ‚îÇ
                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ          ‚îÇ
                                                        ‚îÇ          ‚îÇ
                                                        ‚ñº          ‚ñº
                                                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                                 ‚îÇ   DataWriter     ‚îÇ
                                                 ‚îÇ (train/eval)     ‚îÇ
                                                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                          ‚îÇ
                                                          ‚ñº
                                                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                                 ‚îÇ   DataStorage    ‚îÇ
                                                 ‚îÇ (–≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ)‚îÇ
                                                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

–ü–æ–¥—Ä–æ–±–Ω–µ–µ: [ARCHITECTURE_V2.md](ARCHITECTURE_V2.md)

---

## üíæ –£—Å—Ç–∞–Ω–æ–≤–∫–∞

### –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è

- Python 3.10+
- pip / poetry

### –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏

```txt
# Core
python-telegram-bot==21.6
pydantic>=2.8
pydantic-ai>=0.0.14
pydantic-settings>=2.5

# LLM
openai>=1.40.0
tiktoken>=0.7.0
ollama>=0.1.0  # Optional

# Data
pandas>=2.2
openpyxl>=3.1
pyarrow>=15.0

# Utils
httpx>=0.27
python-dotenv>=1.0
tqdm>=4.66.0
rich>=13.7.0
loguru>=0.7.0
```

### –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å Poetry

```bash
poetry install
poetry shell
```

### –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å pip

```bash
pip install -r requirements.txt
```

---

## ‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

### –°—Ç—Ä—É–∫—Ç—É—Ä–∞ .env

```env
# ========== Telegram ==========
TELEGRAM_BOT_TOKEN=1234567890:ABC...
TELEGRAM_PUBLIC_URL=https://your-app.railway.app
TELEGRAM_PORT=8080

# ========== LLM (–æ—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å) ==========
LLM_API_KEY=sk-...
LLM_API_BASE=https://api.openai.com/v1
LLM_MODEL=gpt-4o-mini
LLM_TEMPERATURE=1.0

# ========== LLM Labeler (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ - —Å–≤–æ—è –º–æ–¥–µ–ª—å) ==========
# –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ—Å–Ω–æ–≤–Ω—É—é –º–æ–¥–µ–ª—å
LLM_LABELER_API_KEY=sk-...
LLM_LABELER_API_BASE=http://localhost:11434/v1  # Ollama
LLM_LABELER_MODEL=llama3.1:8b

# ========== LLM Augmenter (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) ==========
LLM_AUGMENTER_API_KEY=sk-...
LLM_AUGMENTER_MODEL=gpt-4o-mini

# ========== ETL ==========
ETL_MAX_ROWS=10000
ETL_DEDUPLICATE=true
ETL_MIN_TEXT_LENGTH=3
ETL_MAX_TEXT_LENGTH=1000

# ========== Labeler ==========
LABELER_BATCH_SIZE=20
LABELER_RATE_LIMIT=0.4
LABELER_LOW_CONF_THRESHOLD=0.5
LABELER_USE_CACHE=true
LABELER_USE_DYNAMIC_FEWSHOT=true

# ========== Augmenter ==========
AUGMENTER_VARIANTS_PER_SAMPLE=3
AUGMENTER_INCLUDE_HARD_NEGATIVES=false
AUGMENTER_CONCURRENCY=8
AUGMENTER_RATE_LIMIT=0.1
AUGMENTER_MAX_SAMPLES_PER_DOMAIN=30

# ========== Review (HITL) ==========
REVIEW_LOW_CONFIDENCE_THRESHOLD=0.5
REVIEW_HIGH_PRIORITY_THRESHOLD=0.3
REVIEW_MAX_QUEUE_SIZE=10000
REVIEW_AUTO_APPROVE_THRESHOLD=0.95

# ========== DataWriter ==========
DATA_WRITER_EVAL_FRACTION=0.1
DATA_WRITER_MIN_EVAL_SAMPLES=50
DATA_WRITER_BALANCE_DOMAINS=true
DATA_WRITER_VALIDATE_QUALITY=true

# ========== DataStorage ==========
DATA_STORAGE_MAX_VERSIONS=100
DATA_STORAGE_AUTO_ARCHIVE_OLD=true
DATA_STORAGE_ENABLE_COMPRESSION=false

# ========== Cache ==========
CACHE_TTL_HOURS=24
CACHE_ENABLED=true

# ========== App ==========
APP_MODE=production  # –∏–ª–∏ development
APP_DATA_DIR=data
APP_LOG_LEVEL=INFO
APP_LOG_TO_FILE=false
```

### –ü—Ä–æ–≥—Ä–∞–º–º–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

```python
from src.config_v2 import Settings

settings = Settings.load()

# –î–æ—Å—Ç—É–ø –∫ –∫–æ–Ω—Ñ–∏–≥—É
print(settings.telegram.bot_token)
print(settings.llm.model)
print(settings.labeler.batch_size)

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∂–∏–º–∞
if settings.is_production():
    print("Running in production mode")
```

---

## üéØ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

### Telegram Bot

```bash
# –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞
python -m src.bot

# –ö–æ–º–∞–Ω–¥—ã –≤ –±–æ—Ç–µ:
/start - –ù–∞—á–∞–ª–æ —Ä–∞–±–æ—Ç—ã
/menu - –ì–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é
/stats - –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
/help - –ü–æ–º–æ—â—å

# –û—Ç–ø—Ä–∞–≤–∏—Ç—å —Ñ–∞–π–ª .xlsx/.csv –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
# –û—Ç–ø—Ä–∞–≤–∏—Ç—å —Ç–µ–∫—Å—Ç –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
```

### Python API

#### 1. ETL - –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö

```python
from src.pipeline.etl import ETLProcessor, ETLConfig

config = ETLConfig(
    max_rows=10000,
    deduplicate=True,
    min_text_length=3
)

etl = ETLProcessor(config)
df = etl.process_file("data/logs.xlsx")

print(f"Processed {len(df)} rows")
print(df.head())
```

#### 2. Labeler - –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è

```python
from src.pipeline.labeler_agent import LabelerAgent, LabelerConfig

config = LabelerConfig(
    model="gpt-4o-mini",
    api_key="sk-...",
    batch_size=20,
    rate_limit=0.4
)

labeler = LabelerAgent(config)

# –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
result = await labeler.classify_one("–ø–µ—Ä–µ–¥–∞—Ç—å –ø–æ–∫–∞–∑–∞–Ω–∏—è —Å—á–µ—Ç—á–∏–∫–∞")
print(f"Domain: {result.domain_id}, Confidence: {result.confidence}")

# –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è DataFrame
results = await labeler.classify_dataframe(df, text_column="text")

# –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è HITL
low_conf = labeler.get_low_confidence_items(results, threshold=0.5)
print(f"Need review: {len(low_conf)}")
```

#### 3. Augmenter - –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∞—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è

```python
from src.pipeline.augmenter_agent import AugmenterAgent, AugmenterConfig

config = AugmenterConfig(
    model="gpt-4o-mini",
    api_key="sk-...",
    variants_per_sample=3,
    concurrency=8
)

augmenter = AugmenterAgent(config)

# –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –±–∞—Ç—á–∞
items = [{"text": "–ø–µ—Ä–µ–¥–∞—Ç—å –ø–æ–∫–∞–∑–∞–Ω–∏—è", "domain_id": "house"}, ...]
synthetic = await augmenter.augment_batch(items, balance_domains=True)

print(f"Generated {len(synthetic)} synthetic samples")
```

#### 4. ReviewDataset - HITL

```python
from src.pipeline.review_dataset import ReviewDataset, ReviewDatasetConfig

config = ReviewDatasetConfig(
    data_dir=Path("data"),
    low_confidence_threshold=0.5
)

review = ReviewDataset(config)

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –æ—á–µ—Ä–µ–¥—å
review.add_items(low_confidence_items)

# –ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–∞ –ø—Ä–æ–≤–µ—Ä–∫—É
items = review.get_next(count=10, reviewer_id="user123")

# –û—Ç–ø—Ä–∞–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
review.submit_review(
    item_id=items[0].id,
    corrected_domain="house",
    reviewer_id="user123",
    notes="–û—á–µ–≤–∏–¥–Ω–æ –ñ–ö–•"
)

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
stats = review.get_queue_stats()
print(f"Queue size: {stats['queue_size']}")
```

#### 5. DataWriter - –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤

```python
from src.pipeline.data_writer import DataWriter, DataWriterConfig

config = DataWriterConfig(
    output_dir=Path("data/artifacts"),
    eval_fraction=0.1,
    balance_domains=True,
    validate_quality=True
)

writer = DataWriter(config)

# –ó–∞–ø–∏—Å—å –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
train_path, eval_path, stats = writer.write_datasets(
    items=all_items,
    dataset_name="production_v1"
)

print(f"Train: {stats.train_samples}, Eval: {stats.eval_samples}")
print(f"Domains: {stats.domain_distribution}")
print(f"Avg confidence: {stats.avg_confidence:.2f}")
```

#### 6. DataStorage - –í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ

```python
from src.pipeline.data_storage import DataStorage, DataStorageConfig, VersionStatus

config = DataStorageConfig(
    storage_dir=Path("data/storage"),
    max_versions=100,
    auto_archive_old=True
)

storage = DataStorage(config)

# –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ—Ä—Å–∏–∏
version = storage.commit_version(
    train_path=train_path,
    eval_path=eval_path,
    version_tag="v1.2.0",  # –∏–ª–∏ None –¥–ª—è auto-increment
    description="Added feedback, balanced domains",
    status=VersionStatus.STABLE,
    created_by="pipeline_bot"
)

print(f"Created version: {version.version_tag}")

# –¢–µ–≥–∏
storage.tag_version("v1.2.0", "production")
storage.tag_version("v1.2.0", "latest")

# –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –≤–µ—Ä—Å–∏–∏
storage.checkout("v1.2.0")

# –°–ø–∏—Å–æ–∫ –≤–µ—Ä—Å–∏–π
versions = storage.list_versions(status=VersionStatus.STABLE)
for v in versions:
    print(f"{v.version_tag}: {v.description} ({v.created_at})")

# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ
diff = storage.compare_versions("v1.1.0", "v1.2.0")
print(f"Train hash match: {diff['train_hash_match']}")
print(f"Metadata changes: {diff['metadata_diff']}")

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
stats = storage.get_stats()
print(f"Total versions: {stats['total_versions']}")
print(f"Storage size: {stats['total_size_mb']:.2f} MB")
```

---

## üåê –õ–æ–∫–∞–ª—å–Ω—ã–µ LLM

### Ollama

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞
curl -fsSL https://ollama.ai/install.sh | sh

# –ó–∞–ø—É—Å–∫
ollama serve

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
ollama pull llama3.1:8b

# .env –∫–æ–Ω—Ñ–∏–≥
LLM_API_BASE=http://localhost:11434/v1
LLM_MODEL=llama3.1:8b
LLM_API_KEY=dummy
```

**–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:**
- `llama3.1:8b` - —Ö–æ—Ä–æ—à–∏–π –±–∞–ª–∞–Ω—Å –∫–∞—á–µ—Å—Ç–≤–∞/—Å–∫–æ—Ä–æ—Å—Ç–∏
- `mistral:7b` - –±—ã—Å—Ç—Ä–∞—è, —Ö–æ—Ä–æ—à–æ —Å–ª–µ–¥—É–µ—Ç –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º
- `qwen2:7b` - –æ—Ç–ª–∏—á–Ω–∞—è –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞

### vLLM

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞
pip install vllm

# –ó–∞–ø—É—Å–∫
python -m vllm.entrypoints.openai.api_server \
  --model microsoft/DialoGPT-large \
  --port 8000

# .env –∫–æ–Ω—Ñ–∏–≥
LLM_API_BASE=http://localhost:8000/v1
LLM_MODEL=microsoft/DialoGPT-large
```

### LM Studio

1. –°–∫–∞—á–∞–π—Ç–µ –∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ [LM Studio](https://lmstudio.ai/)
2. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ GUI
3. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –ª–æ–∫–∞–ª—å–Ω—ã–π —Å–µ—Ä–≤–µ—Ä
4. –ù–∞—Å—Ç—Ä–æ–π—Ç–µ `.env`:

```env
LLM_API_BASE=http://localhost:1234/v1
LLM_MODEL=local-model
LLM_API_KEY=dummy
```

### –°–º–µ—à–∞–Ω–Ω—ã–π —Ä–µ–∂–∏–º

–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è Labeler –∏ –æ–±–ª–∞—á–Ω—ã–µ –¥–ª—è Augmenter:

```env
# Labeler - –ª–æ–∫–∞–ª—å–Ω–æ (–±—ã—Å—Ç—Ä–æ, –±–µ—Å–ø–ª–∞—Ç–Ω–æ)
LLM_LABELER_API_BASE=http://localhost:11434/v1
LLM_LABELER_MODEL=llama3.1:8b
LLM_LABELER_API_KEY=dummy

# Augmenter - OpenAI (–∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏)
LLM_AUGMENTER_API_KEY=sk-...
LLM_AUGMENTER_MODEL=gpt-4o-mini
```

---

## üìö API Reference

### ETLProcessor

```python
class ETLProcessor:
    def __init__(self, config: ETLConfig)
    
    def process_file(self, file_path: Path, source_name: Optional[str] = None) -> pd.DataFrame
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ñ–∞–π–ª –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç DataFrame.
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã: XLSX, CSV, JSON, JSONL, Parquet
    
    Returns:
        DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏: text, ts, user_id, source, metadata
    """
    
    def get_stats(self) -> Dict[str, Any]
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
```

### LabelerAgent

```python
class LabelerAgent:
    def __init__(self, config: LabelerConfig)
    
    async def classify_one(
        self,
        text: str,
        *,
        allowed_labels: Optional[List[str]] = None,
        user_context: Optional[str] = None
    ) -> ClassificationResult
    """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç"""
    
    async def classify_batch(
        self,
        texts: List[str],
        *,
        progress_callback: Optional[callable] = None
    ) -> List[ClassificationResult]
    """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –±–∞—Ç—á —Ç–µ–∫—Å—Ç–æ–≤ —Å rate limiting"""
    
    async def classify_dataframe(
        self,
        df: pd.DataFrame,
        text_column: str = "text",
        *,
        progress_callback: Optional[callable] = None
    ) -> List[ClassificationResult]
    """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç DataFrame"""
    
    def get_low_confidence_items(
        self,
        results: List[ClassificationResult],
        threshold: Optional[float] = None
    ) -> List[ClassificationResult]
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —ç–ª–µ–º–µ–Ω—Ç—ã —Å –Ω–∏–∑–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é"""
    
    def get_stats(self) -> Dict[str, Any]
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ä–∞–±–æ—Ç—ã –∞–≥–µ–Ω—Ç–∞"""
```

### AugmenterAgent

```python
class AugmenterAgent:
    def __init__(self, config: AugmenterConfig)
    
    async def augment_one(self, text: str, domain: str) -> AugmentationResult
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤–∞—Ä–∏–∞–Ω—Ç—ã –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
    
    async def augment_batch(
        self,
        items: List[Dict[str, Any]],
        *,
        balance_domains: bool = True,
        progress_callback: Optional[callable] = None
    ) -> List[AugmentedSample]
    """–ê—É–≥–º–µ–Ω—Ç–∏—Ä—É–µ—Ç –±–∞—Ç—á —Ç–µ–∫—Å—Ç–æ–≤"""
    
    def get_stats(self) -> Dict[str, Any]
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
```

### ReviewDataset

```python
class ReviewDataset:
    def __init__(self, config: ReviewDatasetConfig)
    
    def add_items(self, items: List[Dict[str, Any]]) -> int
    """–î–æ–±–∞–≤–ª—è–µ—Ç —ç–ª–µ–º–µ–Ω—Ç—ã –≤ –æ—á–µ—Ä–µ–¥—å"""
    
    def get_next(self, count: int = 1, reviewer_id: Optional[str] = None) -> List[ReviewItem]
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–µ–¥—É—é—â–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏"""
    
    def submit_review(
        self,
        item_id: str,
        corrected_domain: str,
        reviewer_id: Optional[str] = None,
        notes: Optional[str] = None
    ) -> bool
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏"""
    
    def get_queue_stats(self) -> Dict[str, Any]
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ—á–µ—Ä–µ–¥–∏"""
    
    def export_reviewed(self, output_path: Optional[Path] = None) -> Path
    """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã"""
```

### DataWriter

```python
class DataWriter:
    def __init__(self, config: DataWriterConfig)
    
    def write_datasets(
        self,
        items: List[Dict[str, Any]],
        *,
        dataset_name: str = "dataset"
    ) -> Tuple[Path, Path, DatasetStats]
    """
    –ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç train –∏ eval –¥–∞—Ç–∞—Å–µ—Ç—ã.
    
    Returns:
        Tuple[train_path, eval_path, stats]
    """
    
    def get_last_stats(self) -> Optional[DatasetStats]
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ—Å–ª–µ–¥–Ω–µ–π –∑–∞–ø–∏—Å–∏"""
```

### DataStorage

```python
class DataStorage:
    def __init__(self, config: DataStorageConfig)
    
    def commit_version(
        self,
        train_path: Path,
        eval_path: Path,
        *,
        version_tag: Optional[str] = None,
        description: Optional[str] = None,
        status: VersionStatus = VersionStatus.DRAFT,
        metadata: Optional[Dict[str, Any]] = None,
        increment_type: str = "minor",
        created_by: Optional[str] = None
    ) -> Version
    """–°–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—É—é –≤–µ—Ä—Å–∏—é –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    
    def checkout(self, version_tag: str) -> bool
    """–ü–µ—Ä–µ–∫–ª—é—á–∞–µ—Ç—Å—è –Ω–∞ —É–∫–∞–∑–∞–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é"""
    
    def tag_version(self, version_tag: str, tag: str) -> bool
    """–î–æ–±–∞–≤–ª—è–µ—Ç —Ç–µ–≥ –∫ –≤–µ—Ä—Å–∏–∏"""
    
    def list_versions(
        self,
        status: Optional[VersionStatus] = None,
        tag: Optional[str] = None
    ) -> List[Version]
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤–µ—Ä—Å–∏–π"""
    
    def compare_versions(self, version_tag1: str, version_tag2: str) -> Dict[str, Any]
    """–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –¥–≤–µ –≤–µ—Ä—Å–∏–∏"""
    
    def get_stats(self) -> Dict[str, Any]
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ö—Ä–∞–Ω–∏–ª–∏—â–∞"""
```

---

## üí° –ü—Ä–∏–º–µ—Ä—ã

### –ü–æ–ª–Ω—ã–π pipeline

```python
import asyncio
from pathlib import Path
from src.pipeline import *
from src.config_v2 import Settings

async def full_pipeline():
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    settings = Settings.load()
    
    # 1. ETL - –æ–±—Ä–∞–±–æ—Ç–∫–∞
    print("üì• ETL Processing...")
    etl = ETLProcessor(ETLConfig(max_rows=10000))
    df = etl.process_file(Path("data/logs.xlsx"))
    print(f"‚úÖ Processed: {len(df)} rows")
    
    # 2. Labeler - —Ä–∞–∑–º–µ—Ç–∫–∞
    print("üè∑Ô∏è  Labeling...")
    labeler_config = LabelerConfig(
        **settings.get_labeler_llm_config(),
        batch_size=20,
        rate_limit=0.4
    )
    labeler = LabelerAgent(labeler_config)
    results = await labeler.classify_dataframe(df)
    print(f"‚úÖ Labeled: {len(results)} texts")
    
    # 3. Review - HITL
    print("üë§ HITL Review...")
    low_conf = labeler.get_low_confidence_items(results, threshold=0.5)
    if low_conf:
        review = ReviewDataset(ReviewDatasetConfig(data_dir=Path("data")))
        review.add_items([r.dict() for r in low_conf])
        print(f"‚è≥ Added {len(low_conf)} items to review queue")
    
    # 4. Augmenter - —Å–∏–Ω—Ç–µ—Ç–∏–∫–∞
    print("üß¨ Augmentation...")
    high_conf = [r for r in results if r.confidence >= 0.7]
    augmenter_config = AugmenterConfig(
        **settings.get_augmenter_llm_config(),
        variants_per_sample=3
    )
    augmenter = AugmenterAgent(augmenter_config)
    synthetic = await augmenter.augment_batch(
        [r.dict() for r in high_conf[:100]]  # –õ–∏–º–∏—Ç–∏—Ä—É–µ–º –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
    )
    print(f"‚úÖ Generated: {len(synthetic)} synthetic samples")
    
    # 5. DataWriter - —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    print("üíæ Writing datasets...")
    all_items = [r.dict() for r in results] + [s.dict() for s in synthetic]
    writer = DataWriter(DataWriterConfig(
        output_dir=Path("data/artifacts"),
        eval_fraction=0.1,
        balance_domains=True
    ))
    train_path, eval_path, stats = writer.write_datasets(
        all_items,
        dataset_name="production"
    )
    print(f"‚úÖ Train: {stats.train_samples}, Eval: {stats.eval_samples}")
    
    # 6. DataStorage - –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
    print("üì¶ Versioning...")
    storage = DataStorage(DataStorageConfig(
        storage_dir=Path("data/storage")
    ))
    version = storage.commit_version(
        train_path=train_path,
        eval_path=eval_path,
        description="Automated pipeline run with synthetic augmentation",
        status=VersionStatus.STABLE
    )
    storage.tag_version(version.version_tag, "latest")
    print(f"‚úÖ Version: {version.version_tag}")
    
    print("\nüéâ Pipeline completed successfully!")

if __name__ == "__main__":
    asyncio.run(full_pipeline())
```

---

## üöÄ –î–µ–ø–ª–æ–π

### Railway

1. **Push –≤ GitHub**
   ```bash
   git add .
   git commit -m "Ready for deploy"
   git push origin main
   ```

2. **Railway Deploy**
   - Connect GitHub repo
   - Add variables (—Å–º. `.env`)
   - Start Command: `python -m src.bot`

3. **–ù–∞—Å—Ç—Ä–æ–π–∫–∞**
   ```env
   TELEGRAM_PUBLIC_URL=https://your-app.railway.app
   APP_MODE=production
   ```

### Docker

```dockerfile
FROM python:3.10-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["python", "-m", "src.bot"]
```

```bash
docker build -t esk-pipeline .
docker run -d --env-file .env esk-pipeline
```

---

## ü§ù Contributing

–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é—Ç—Å—è pull requests!

### –ü—Ä–æ—Ü–µ—Å—Å

1. Fork —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è
2. –°–æ–∑–¥–∞–π—Ç–µ feature branch (`git checkout -b feature/amazing-feature`)
3. Commit –∏–∑–º–µ–Ω–µ–Ω–∏–π (`git commit -m 'Add amazing feature'`)
4. Push –≤ branch (`git push origin feature/amazing-feature`)
5. –û—Ç–∫—Ä–æ–π—Ç–µ Pull Request

### –°—Ç–∏–ª—å –∫–æ–¥–∞

- Black –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
- Pydantic –¥–ª—è –≤—Å–µ—Ö –∫–æ–Ω—Ñ–∏–≥–æ–≤
- Type hints –≤–µ–∑–¥–µ
- Docstrings –¥–ª—è –ø—É–±–ª–∏—á–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤

---

## üìÑ –õ–∏—Ü–µ–Ω–∑–∏—è

MIT License - —Å–º. [LICENSE](LICENSE)

---

## üôè –ë–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç–∏

- [PydanticAI](https://ai.pydantic.dev/) - —Ç–∏–ø–æ–±–µ–∑–æ–ø–∞—Å–Ω—ã–µ AI-–∞–≥–µ–Ω—Ç—ã
- [Pydantic](https://docs.pydantic.dev/) - –≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
- [python-telegram-bot](https://python-telegram-bot.org/) - Telegram API
- [Ollama](https://ollama.ai/) - –ª–æ–∫–∞–ª—å–Ω—ã–µ LLM –º–æ–¥–µ–ª–∏

---

## üìû –ü–æ–¥–¥–µ—Ä–∂–∫–∞

- üìß Email: your-email@example.com
- üí¨ Telegram: @your_username
- üêõ Issues: [GitHub Issues](https://github.com/your-username/esk-agent-llm-pro/issues)

---

**Built with ‚ù§Ô∏è for Production ML**

