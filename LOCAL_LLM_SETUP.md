# üåê –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–∫–∞–ª—å–Ω—ã—Ö LLM –≤ –∑–∞–∫—Ä—ã—Ç–æ–º –∫–æ–Ω—Ç—É—Ä–µ

–†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—é Mistral, Qwen –∏ –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å pipeline.

---

## üéØ –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∑–∞–∫—Ä—ã—Ç–æ–≥–æ –∫–æ–Ω—Ç—É—Ä–∞

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         –ó–∞–∫—Ä—ã—Ç—ã–π –∫–æ–Ω—Ç—É—Ä (–≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —Å–µ—Ç—å)        ‚îÇ
‚îÇ                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ ML Pipeline  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ LLM Instance    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   (API)      ‚îÇ  HTTP   ‚îÇ (Mistral/Qwen)  ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ        ‚Üì                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                               ‚îÇ
‚îÇ  ‚îÇ Data Storage ‚îÇ                               ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**–ù–µ—Ç –≤–Ω–µ—à–Ω–∏—Ö API calls!** –í—Å—ë —Ä–∞–±–æ—Ç–∞–µ—Ç –≤–Ω—É—Ç—Ä–∏ –≤–∞—à–µ–π –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã.

---

## üöÄ –í–∞—Ä–∏–∞–Ω—Ç—ã —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è

### –í–∞—Ä–∏–∞–Ω—Ç 1: vLLM (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è production)

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- ‚úÖ –í—ã—Å–æ–∫–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
- ‚úÖ Batch processing –∏–∑ –∫–æ—Ä–æ–±–∫–∏
- ‚úÖ OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π API
- ‚úÖ –õ–µ–≥–∫–æ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç—Å—è

#### –†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ:

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞
pip install vllm

# –ó–∞–ø—É—Å–∫ Mistral
python -m vllm.entrypoints.openai.api_server \
  --model mistralai/Mistral-7B-Instruct-v0.2 \
  --host 0.0.0.0 \
  --port 8000 \
  --dtype float16 \
  --max-model-len 8192

# –ó–∞–ø—É—Å–∫ Qwen
python -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen2-7B-Instruct \
  --host 0.0.0.0 \
  --port 8001 \
  --dtype float16 \
  --max-model-len 8192
```

#### Docker –¥–ª—è vLLM:

```yaml
# docker-compose.yml
services:
  vllm-mistral:
    image: vllm/vllm-openai:latest
    command: >
      --model mistralai/Mistral-7B-Instruct-v0.2
      --host 0.0.0.0
      --port 8000
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
  
  vllm-qwen:
    image: vllm/vllm-openai:latest
    command: >
      --model Qwen/Qwen2-7B-Instruct
      --host 0.0.0.0
      --port 8001
    ports:
      - "8001:8001"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
  
  ml-pipeline:
    build: .
    ports:
      - "8080:8000"
    environment:
      # Labeler –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Mistral
      - LLM_LABELER_API_BASE=http://vllm-mistral:8000/v1
      - LLM_LABELER_MODEL=mistralai/Mistral-7B-Instruct-v0.2
      - LLM_LABELER_API_KEY=dummy
      
      # Augmenter –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Qwen
      - LLM_AUGMENTER_API_BASE=http://vllm-qwen:8001/v1
      - LLM_AUGMENTER_MODEL=Qwen/Qwen2-7B-Instruct
      - LLM_AUGMENTER_API_KEY=dummy
    depends_on:
      - vllm-mistral
      - vllm-qwen
```

---

### –í–∞—Ä–∏–∞–Ω—Ç 2: Ollama (–ø—Ä–æ—â–µ –¥–ª—è dev/test)

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- ‚úÖ –ü—Ä–æ—Å—Ç–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞
- ‚úÖ –£–¥–æ–±–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª—è–º–∏
- ‚úÖ GUI –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ

#### –†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ:

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞
curl -fsSL https://ollama.ai/install.sh | sh

# –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞
ollama serve

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π
ollama pull mistral:7b
ollama pull qwen2:7b

# –ü—Ä–æ–≤–µ—Ä–∫–∞
ollama list
```

#### Docker –¥–ª—è Ollama:

```yaml
# docker-compose.yml
services:
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    restart: unless-stopped
  
  ml-pipeline:
    build: .
    ports:
      - "8080:8000"
    environment:
      # –ò—Å–ø–æ–ª—å–∑—É–µ–º Ollama –¥–ª—è –≤—Å–µ–≥–æ
      - LLM_API_BASE=http://ollama:11434/v1
      - LLM_MODEL=mistral:7b
      - LLM_API_KEY=dummy
      
      # –ò–ª–∏ —Ä–∞–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏:
      - LLM_LABELER_API_BASE=http://ollama:11434/v1
      - LLM_LABELER_MODEL=mistral:7b
      - LLM_LABELER_API_KEY=dummy
      
      - LLM_AUGMENTER_API_BASE=http://ollama:11434/v1
      - LLM_AUGMENTER_MODEL=qwen2:7b
      - LLM_AUGMENTER_API_KEY=dummy
    depends_on:
      - ollama

volumes:
  ollama_data:
```

–ü–æ—Å–ª–µ –∑–∞–ø—É—Å–∫–∞:
```bash
# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä
docker-compose exec ollama ollama pull mistral:7b
docker-compose exec ollama ollama pull qwen2:7b
```

---

### –í–∞—Ä–∏–∞–Ω—Ç 3: Text Generation WebUI (GUI + API)

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- ‚úÖ –í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è
- ‚úÖ –ú–Ω–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
- ‚úÖ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –º–æ–¥–µ–ª–µ–π

#### –†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ:

```bash
# –ö–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
git clone https://github.com/oobabooga/text-generation-webui
cd text-generation-webui

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞
pip install -r requirements.txt

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ (—á–µ—Ä–µ–∑ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –∏–ª–∏ CLI)
python download-model.py mistralai/Mistral-7B-Instruct-v0.2

# –ó–∞–ø—É—Å–∫ —Å API
python server.py \
  --api \
  --listen \
  --model mistralai_Mistral-7B-Instruct-v0.2
```

–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:
```env
LLM_API_BASE=http://localhost:5000/v1
LLM_MODEL=mistralai_Mistral-7B-Instruct-v0.2
LLM_API_KEY=dummy
```

---

## üîß –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è pipeline

### .env –¥–ª—è –∑–∞–∫—Ä—ã—Ç–æ–≥–æ –∫–æ–Ω—Ç—É—Ä–∞:

```env
# ========== –î–ª—è vLLM ==========

# Labeler - Mistral (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è)
LLM_LABELER_API_BASE=http://your-vllm-server:8000/v1
LLM_LABELER_MODEL=mistralai/Mistral-7B-Instruct-v0.2
LLM_LABELER_API_KEY=dummy

# Augmenter - Qwen (–≥–µ–Ω–µ—Ä–∞—Ü–∏—è)
LLM_AUGMENTER_API_BASE=http://your-vllm-server:8001/v1
LLM_AUGMENTER_MODEL=Qwen/Qwen2-7B-Instruct
LLM_AUGMENTER_API_KEY=dummy

# ========== –î–ª—è Ollama ==========

# –û–±–µ –∑–∞–¥–∞—á–∏ —á–µ—Ä–µ–∑ –æ–¥–Ω—É –º–æ–¥–µ–ª—å
LLM_API_BASE=http://your-ollama-server:11434/v1
LLM_MODEL=mistral:7b
LLM_API_KEY=dummy

# –ò–ª–∏ —Ä–∞–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏:
LLM_LABELER_API_BASE=http://your-ollama-server:11434/v1
LLM_LABELER_MODEL=mistral:7b

LLM_AUGMENTER_API_BASE=http://your-ollama-server:11434/v1
LLM_AUGMENTER_MODEL=qwen2:7b

# ========== –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π ==========

# –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º rate limit (–ª–æ–∫–∞–ª—å–Ω–æ –±—ã—Å—Ç—Ä–µ–µ)
LABELER_RATE_LIMIT=0.2
AUGMENTER_RATE_LIMIT=0.05

# –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º batch size
LABELER_BATCH_SIZE=50

# –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º concurrency
AUGMENTER_CONCURRENCY=16
```

---

## üéØ –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∑–∞–¥–∞—á

### –î–ª—è Labeler (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è):

| –ú–æ–¥–µ–ª—å | –†–∞–∑–º–µ—Ä | –°–∫–æ—Ä–æ—Å—Ç—å | –ö–∞—á–µ—Å—Ç–≤–æ | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è |
|--------|--------|----------|----------|--------------|
| **Mistral-7B-Instruct-v0.2** | 7B | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ –õ—É—á—à–∏–π –≤—ã–±–æ—Ä |
| **Qwen2-7B-Instruct** | 7B | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ –û—Ç–ª–∏—á–Ω–æ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ |
| **Llama-3.1-8B-Instruct** | 8B | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ –í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ |
| **Phi-3-mini** | 3.8B | ‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | üí° –ï—Å–ª–∏ –º–∞–ª–æ —Ä–µ—Å—É—Ä—Å–æ–≤ |

### –î–ª—è Augmenter (–≥–µ–Ω–µ—Ä–∞—Ü–∏—è):

| –ú–æ–¥–µ–ª—å | –†–∞–∑–º–µ—Ä | –ö—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å | –ö–∞—á–µ—Å—Ç–≤–æ | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è |
|--------|--------|--------------|----------|--------------|
| **Qwen2-7B-Instruct** | 7B | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ –õ—É—á—à–∏–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ |
| **Mistral-7B-Instruct-v0.2** | 7B | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ –•–æ—Ä–æ—à–∏–π –±–∞–ª–∞–Ω—Å |
| **Llama-3.1-8B-Instruct** | 8B | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ –í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ |

---

## üîí –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –≤ –∑–∞–∫—Ä—ã—Ç–æ–º –∫–æ–Ω—Ç—É—Ä–µ

### 1. –ò–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–µ—Ç—å Docker

```yaml
# docker-compose.yml
networks:
  internal:
    driver: bridge
    internal: true  # –ù–µ—Ç –≤—ã—Ö–æ–¥–∞ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç!

services:
  vllm-mistral:
    networks:
      - internal
  
  ml-pipeline:
    networks:
      - internal
    # –¢–æ–ª—å–∫–æ —ç—Ç–æ—Ç —Å–µ—Ä–≤–∏—Å –∏–º–µ–µ—Ç –¥–æ—Å—Ç—É–ø –Ω–∞—Ä—É–∂—É (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
    ports:
      - "8080:8000"
```

### 2. –ë–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

```env
# –ù–ï–¢ OpenAI API
# –ù–ï–¢ –≤–Ω–µ—à–Ω–∏—Ö —Å–µ—Ä–≤–∏—Å–æ–≤
# –í–°–Å —Ä–∞–±–æ—Ç–∞–µ—Ç –≤–Ω—É—Ç—Ä–∏ –∫–æ–Ω—Ç—É—Ä–∞

LLM_LABELER_API_BASE=http://vllm-mistral:8000/v1
LLM_AUGMENTER_API_BASE=http://vllm-qwen:8001/v1
```

### 3. Firewall –ø—Ä–∞–≤–∏–ª–∞

```bash
# –†–∞–∑—Ä–µ—à–∞–µ–º —Ç–æ–ª—å–∫–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
iptables -A INPUT -s 172.16.0.0/12 -j ACCEPT  # Docker —Å–µ—Ç—å
iptables -A INPUT -j DROP  # –û—Å—Ç–∞–ª—å–Ω–æ–µ –±–ª–æ–∫–∏—Ä—É–µ–º
```

---

## ‚öôÔ∏è –ü–æ–ª–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –∑–∞–∫—Ä—ã—Ç–æ–≥–æ –∫–æ–Ω—Ç—É—Ä–∞

### docker-compose.yml (production):

```yaml
version: '3.8'

networks:
  ml-network:
    driver: bridge
    internal: false  # true –µ—Å–ª–∏ –ø–æ–ª–Ω–∞—è –∏–∑–æ–ª—è—Ü–∏—è

services:
  # Mistral –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
  llm-labeler:
    image: vllm/vllm-openai:latest
    container_name: llm-labeler
    command: >
      --model mistralai/Mistral-7B-Instruct-v0.2
      --host 0.0.0.0
      --port 8000
      --dtype float16
      --max-model-len 8192
      --gpu-memory-utilization 0.9
    ports:
      - "8000:8000"  # –¢–æ–ª—å–∫–æ –¥–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π —Å–µ—Ç–∏
    volumes:
      - huggingface_cache:/root/.cache/huggingface
    networks:
      - ml-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
  
  # Qwen –¥–ª—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
  llm-augmenter:
    image: vllm/vllm-openai:latest
    container_name: llm-augmenter
    command: >
      --model Qwen/Qwen2-7B-Instruct
      --host 0.0.0.0
      --port 8001
      --dtype float16
      --max-model-len 8192
      --gpu-memory-utilization 0.9
    ports:
      - "8001:8001"  # –¢–æ–ª—å–∫–æ –¥–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π —Å–µ—Ç–∏
    volumes:
      - huggingface_cache:/root/.cache/huggingface
    networks:
      - ml-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
  
  # ML Pipeline
  ml-pipeline:
    build: .
    container_name: ml-pipeline
    ports:
      - "8080:8000"  # –ï–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –≤–Ω–µ—à–Ω–∏–π –ø–æ—Ä—Ç
    environment:
      # Labeler ‚Üí Mistral
      - LLM_LABELER_API_BASE=http://llm-labeler:8000/v1
      - LLM_LABELER_MODEL=mistralai/Mistral-7B-Instruct-v0.2
      - LLM_LABELER_API_KEY=dummy
      
      # Augmenter ‚Üí Qwen
      - LLM_AUGMENTER_API_BASE=http://llm-augmenter:8001/v1
      - LLM_AUGMENTER_MODEL=Qwen/Qwen2-7B-Instruct
      - LLM_AUGMENTER_API_KEY=dummy
      
      # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
      - LABELER_BATCH_SIZE=50
      - LABELER_RATE_LIMIT=0.1
      - AUGMENTER_CONCURRENCY=16
      - AUGMENTER_RATE_LIMIT=0.05
      
      # Quality Control
      - QC_MIN_COSINE_SIMILARITY=0.3
      - QC_MAX_COSINE_SIMILARITY=0.95
      - QC_STRICT_MODE=true
      
      # Cache
      - CACHE_TTL_HOURS=168  # 7 –¥–Ω–µ–π –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
      - CACHE_ENABLED=true
      
      # App
      - APP_MODE=production
      - APP_LOG_LEVEL=INFO
    volumes:
      - ./data:/app/data
    networks:
      - ml-network
    depends_on:
      llm-labeler:
        condition: service_healthy
      llm-augmenter:
        condition: service_healthy
    restart: unless-stopped

volumes:
  huggingface_cache:
    driver: local
```

---

## üöÄ –ó–∞–ø—É—Å–∫ –∑–∞–∫—Ä—ã—Ç–æ–≥–æ –∫–æ–Ω—Ç—É—Ä–∞

### 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞

```bash
# –°–æ–∑–¥–∞–π—Ç–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞
cd /path/to/esk-agent-llm-pro

# –°–∫–æ–ø–∏—Ä—É–π—Ç–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
cp env.docker.example .env
```

### 2. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è `.env`:

```env
# –ù–ï–¢ OpenAI API!
# –¢–æ–ª—å–∫–æ –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏

# Labeler - Mistral
LLM_LABELER_API_BASE=http://llm-labeler:8000/v1
LLM_LABELER_MODEL=mistralai/Mistral-7B-Instruct-v0.2
LLM_LABELER_API_KEY=dummy

# Augmenter - Qwen
LLM_AUGMENTER_API_BASE=http://llm-augmenter:8001/v1
LLM_AUGMENTER_MODEL=Qwen/Qwen2-7B-Instruct
LLM_AUGMENTER_API_KEY=dummy

# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö
LABELER_BATCH_SIZE=50
LABELER_RATE_LIMIT=0.1
AUGMENTER_CONCURRENCY=16
CACHE_TTL_HOURS=168
```

### 3. –ó–∞–ø—É—Å–∫

```bash
# –ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ - —Å–∫–∞—á–∞–µ—Ç –º–æ–¥–µ–ª–∏ (–º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è)
docker-compose up -d

# –õ–æ–≥–∏
docker-compose logs -f

# –ü—Ä–æ–≤–µ—Ä–∫–∞
curl http://localhost:8080/health
```

### 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–¥–µ–ª–µ–π

```bash
# Mistral
curl http://localhost:8000/v1/models

# Qwen
curl http://localhost:8001/v1/models

# Pipeline
curl http://localhost:8080/health
```

---

## üìä –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

### –û–∂–∏–¥–∞–µ–º–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å (7B –º–æ–¥–µ–ª–∏ –Ω–∞ GPU):

| –û–ø–µ—Ä–∞—Ü–∏—è | vLLM | Ollama | –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ |
|----------|------|--------|------------|
| –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è 1 —Ç–µ–∫—Å—Ç–∞ | ~0.5s | ~1s | Mistral |
| Batch 50 —Ç–µ–∫—Å—Ç–æ–≤ | ~10s | ~25s | –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ |
| –ì–µ–Ω–µ—Ä–∞—Ü–∏—è 3 –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ | ~2s | ~4s | Qwen |
| –û–±—Ä–∞–±–æ—Ç–∫–∞ 1000 –ª–æ–≥–æ–≤ | ~5 –º–∏–Ω | ~12 –º–∏–Ω | –ü–æ–ª–Ω—ã–π pipeline |

### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è:

```env
# –î–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏ (vLLM)
LABELER_BATCH_SIZE=100
LABELER_RATE_LIMIT=0.05
AUGMENTER_CONCURRENCY=32
AUGMENTER_RATE_LIMIT=0.02

# –î–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏ (Ollama)
LABELER_BATCH_SIZE=20
LABELER_RATE_LIMIT=0.3
AUGMENTER_CONCURRENCY=4
```

---

## üéØ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è production

### 1. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π

```
Labeler  ‚Üí Mistral-7B  (–±—ã—Å—Ç—Ä–∞—è, —Ç–æ—á–Ω–∞—è –¥–ª—è classification)
Augmenter ‚Üí Qwen2-7B   (–∫—Ä–µ–∞—Ç–∏–≤–Ω–∞—è –¥–ª—è generation)
```

**–ü–æ—á–µ–º—É —Ä–∞–∑–Ω—ã–µ:**
- –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç—Ä–µ–±—É–µ—Ç **—Ç–æ—á–Ω–æ—Å—Ç–∏**
- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç—Ä–µ–±—É–µ—Ç **–∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏**

### 2. GPU —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è

| Setup | –ú–æ–¥–µ–ª–∏ | GPU | VRAM |
|-------|--------|-----|------|
| –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π | 1 √ó 7B | 1 √ó RTX 3090 | 24GB |
| –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π | 2 √ó 7B | 2 √ó RTX 3090 | 48GB |
| –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π | 2 √ó 13B | 2 √ó A100 | 80GB |

### 3. –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ

```env
# –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
CACHE_TTL_HOURS=168  # 7 –¥–Ω–µ–π
CACHE_ENABLED=true

# –≠–∫–æ–Ω–æ–º–∏—è:
# - 1000 –ª–æ–≥–æ–≤ ‚Üí ~500 LLM calls (50% cache hit)
# - –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ ‚Üí ~900 cache hits (90%)
```

---

## üîç –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥

### –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è:

```bash
# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ pipeline
curl http://localhost:8080/stats

{
  "labeler": {
    "total_processed": 5000,
    "cache_hits": 2500,
    "llm_calls": 2500,
    "cache_hit_rate": 0.50,
    "avg_latency": 0.52  # —Å–µ–∫—É–Ω–¥—ã
  },
  "quality_control": {
    "passed": 4200,
    "pass_rate": 0.84,
    "rejected_low_similarity": 300,
    "rejected_high_similarity": 150
  }
}
```

### –õ–æ–≥–∏ vLLM:

```bash
# –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å Mistral
docker-compose logs llm-labeler | grep "throughput"

# –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å Qwen
docker-compose logs llm-augmenter | grep "throughput"
```

---

## üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∑–∞–∫—Ä—ã—Ç–æ–º –∫–æ–Ω—Ç—É—Ä–µ

### 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–∑–æ–ª—è—Ü–∏–∏

```bash
# –í–Ω—É—Ç—Ä–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ ml-pipeline –ù–ï –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –¥–æ—Å—Ç—É–ø–∞ –∫ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç—É
docker-compose exec ml-pipeline ping -c 1 google.com
# –û–∂–∏–¥–∞–µ–º: Network is unreachable (–µ—Å–ª–∏ internal: true)

# –ù–æ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –¥–æ—Å—Ç—É–ø –∫ –ª–æ–∫–∞–ª—å–Ω—ã–º LLM
docker-compose exec ml-pipeline curl http://llm-labeler:8000/health
# –û–∂–∏–¥–∞–µ–º: {"status": "ok"}
```

### 2. –¢–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

```bash
# –ó–∞–ø—É—Å–∫ —Å —Ç–µ—Å—Ç–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ (100 –ª–æ–≥–æ–≤)
time curl -X POST "http://localhost:8080/process" \
  -H "Content-Type: application/json" \
  -d '{
    "file_path": "/app/data/uploads/logs_100.csv",
    "augment": true,
    "create_version": true
  }'

# –û–∂–∏–¥–∞–µ–º–æ–µ –≤—Ä–µ–º—è:
# vLLM: ~30-60 —Å–µ–∫—É–Ω–¥
# Ollama: ~2-3 –º–∏–Ω—É—Ç—ã
```

---

## üìù Checklist –¥–ª—è production

- [ ] vLLM —Å–µ—Ä–≤–µ—Ä—ã –∑–∞–ø—É—â–µ–Ω—ã –∏ –¥–æ—Å—Ç—É–ø–Ω—ã
- [ ] –ú–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã (Mistral + Qwen)
- [ ] Pipeline –ø–æ–¥–∫–ª—é—á–∞–µ—Ç—Å—è –∫ –ª–æ–∫–∞–ª—å–Ω—ã–º LLM
- [ ] Health checks –ø—Ä–æ—Ö–æ–¥—è—Ç
- [ ] –¢–µ—Å—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ä–∞–±–æ—Ç–∞–µ—Ç
- [ ] –¢–µ—Å—Ç –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Ä–∞–±–æ—Ç–∞–µ—Ç
- [ ] Quality Control —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ
- [ ] –ù–µ—Ç –≤–Ω–µ—à–Ω–∏—Ö API calls (–ø—Ä–æ–≤–µ—Ä–µ–Ω–æ –ª–æ–≥–∞–º–∏)
- [ ] –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
- [ ] –í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–∑–¥–∞–µ—Ç –≤–µ—Ä—Å–∏–∏
- [ ] –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–∏–µ–º–ª–µ–º–∞—è

---

## üéâ –ì–æ—Ç–æ–≤—ã–π docker-compose –¥–ª—è –∑–∞–∫—Ä—ã—Ç–æ–≥–æ –∫–æ–Ω—Ç—É—Ä–∞

–°–æ–∑–¥–∞–ª –ø–æ–ª–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –≤—ã—à–µ - –ø—Ä–æ—Å—Ç–æ —Å–∫–æ–ø–∏—Ä—É–π—Ç–µ –≤ `docker-compose.yml`.

### –ó–∞–ø—É—Å–∫:

```bash
# 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞
cp env.docker.example .env
# –û—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä—É–π—Ç–µ .env (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ª–æ–∫–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏)

# 2. –ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ (—Å–∫–∞—á–∞–µ—Ç –º–æ–¥–µ–ª–∏)
docker-compose up -d

# 3. –ñ–¥–µ–º –ø–æ–∫–∞ –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∑—è—Ç—Å—è (~10-20 –º–∏–Ω)
docker-compose logs -f llm-labeler
# –î–æ–∂–¥–∏—Ç–µ—Å—å: "Application startup complete"

# 4. –ü—Ä–æ–≤–µ—Ä–∫–∞
curl http://localhost:8080/health

# 5. –¢–µ—Å—Ç
python test_pipeline.py
```

---

## üöÄ –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç—É—Ä–∞

‚úÖ **–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å** - –¥–∞–Ω–Ω—ã–µ –Ω–µ –ø–æ–∫–∏–¥–∞—é—Ç –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—É  
‚úÖ **–°–∫–æ—Ä–æ—Å—Ç—å** - –Ω–µ—Ç —Å–µ—Ç–µ–≤–æ–π –∑–∞–¥–µ—Ä–∂–∫–∏  
‚úÖ **–°—Ç–æ–∏–º–æ—Å—Ç—å** - –±–µ—Å–ø–ª–∞—Ç–Ω–æ –ø–æ—Å–ª–µ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è  
‚úÖ **–ö–æ–Ω—Ç—Ä–æ–ª—å** - –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –Ω–∞–¥ –º–æ–¥–µ–ª—è–º–∏  
‚úÖ **–ü—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å** - –Ω–∏–∫–∞–∫–∏—Ö –≤–Ω–µ—à–Ω–∏—Ö API  
‚úÖ **–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å** - –¥–æ–±–∞–≤–ª—è–π—Ç–µ GPU –ø–æ –º–µ—Ä–µ —Ä–æ—Å—Ç–∞  

---

**–í–∞—à –∑–∞–∫—Ä—ã—Ç—ã–π –∫–æ–Ω—Ç—É—Ä –≥–æ—Ç–æ–≤! üîí‚ú®**

