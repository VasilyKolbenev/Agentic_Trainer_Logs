# src/bot.py
from __future__ import annotations

import json
import logging
from pathlib import Path
from typing import Iterable

from telegram import Update, InputFile
from telegram.ext import (
    Application,
    CommandHandler,
    MessageHandler,
    CallbackQueryHandler,
    ContextTypes,
    filters,
)
from telegram.request import HTTPXRequest

from .config import Settings
from .llm import LLMClient
from .store import Store
from .labeler import label_dataframe_batched, classify_one, low_conf_items, label_dataframe_batched_with_progress
from .augmenter import augment_dataset, split_train_eval, write_jsonl
from .etl import normalize_file_to_df, save_parquet_or_csv  # <‚Äî CSV/XLSX universal
from .ui import main_menu, top_candidates_buttons, hitl_item_buttons
from .adaptive_learning import FeedbackLearner, PromptOptimizer
from .cache import init_cache
from .progress import ProgressTracker, BatchProcessor
from .context import ContextManager

# =========================
# Bootstrap
# =========================
settings = Settings.load()
Path(settings.data_dir).mkdir(parents=True, exist_ok=True)
store = Store(Path(settings.data_dir))

# –°–∏—Å—Ç–µ–º–∞ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
feedback_learner = FeedbackLearner(Path(settings.data_dir))
prompt_optimizer = PromptOptimizer(feedback_learner)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫—ç—à–∞
llm_cache = init_cache(Path(settings.data_dir), ttl_hours=24)

# –ú–µ–Ω–µ–¥–∂–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
context_manager = ContextManager(Path(settings.data_dir))

logging.basicConfig(
    level=getattr(logging, settings.log_level, logging.INFO),
    format="%(asctime)s %(levelname)s %(name)s - %(message)s",
)
logger = logging.getLogger(__name__)

# –†–æ–ª–µ–≤—ã–µ LLM-–∫–ª–∏–µ–Ω—Ç—ã —Å —Ñ–æ–ª–ª–±—ç–∫–æ–º –Ω–∞ –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –∫–ª—é—á–∏
def _mk_client(role: str) -> LLMClient:
    api_key = getattr(settings, f"llm_api_key_{role}", None) or settings.llm_api_key
    api_base = getattr(settings, f"llm_api_base_{role}", None) or settings.llm_api_base
    model   = getattr(settings, f"llm_model_{role}", None) or settings.llm_model
    return LLMClient(api_key=api_key, api_base=api_base, model=model)

label_llm = _mk_client("labeler")
aug_llm   = _mk_client("augmenter")

def get_optimized_prompts():
    """–ü–æ–ª—É—á–∞–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã —Å —É—á–µ—Ç–æ–º feedback"""
    base_system = Path("prompts/labeler_system.txt").read_text(encoding="utf-8")
    base_fewshot = Path("prompts/labeler_fewshot.txt").read_text(encoding="utf-8")
    
    # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
    system_prompt = prompt_optimizer.get_optimized_system_prompt(base_system)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã –∫ –±–∞–∑–æ–≤—ã–º
    dynamic_fewshot = feedback_learner.get_dynamic_fewshot()
    if dynamic_fewshot:
        fewshot_prompt = dynamic_fewshot + "\n" + base_fewshot
    else:
        fewshot_prompt = base_fewshot
        
    return system_prompt, fewshot_prompt

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ–º–ø—Ç—ã
LABELER_SYSTEM, LABELER_FEWSHOT = get_optimized_prompts()
AUG_SYSTEM = Path("prompts/augmenter_system.txt").read_text(encoding="utf-8")

request = HTTPXRequest(
    connect_timeout=10.0, read_timeout=60.0, write_timeout=15.0, pool_timeout=10.0
)
application = Application.builder().token(settings.bot_token).request(request).build()


# =========================
# Helpers
# =========================
async def _send_file_if_nonempty(update: Update, path: Path, *, caption: str | None = None) -> bool:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ—Ç–ø—Ä–∞–≤–∫–∞ —Ñ–∞–π–ª–∞: –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∏ —Ä–∞–∑–º–µ—Ä > 0."""
    try:
        if not path.exists():
            await update.message.reply_text(f"‚ö†Ô∏è –§–∞–π–ª {path.name} –Ω–µ –Ω–∞–π–¥–µ–Ω ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞—é.")
            return False
        if path.stat().st_size == 0:
            await update.message.reply_text(f"‚ö†Ô∏è –§–∞–π–ª {path.name} –ø—É—Å—Ç (0 –±–∞–π—Ç) ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞—é.")
            return False
        with open(path, "rb") as f:
            await update.message.reply_document(document=InputFile(f, filename=path.name), caption=caption)
        return True
    except Exception as e:
        logger.exception("Failed to send file %s", path, exc_info=e)
        await update.message.reply_text(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å {path.name}: {e!s}")
        return False


async def _send_files_batch(update: Update, paths: Iterable[Path]) -> int:
    sent = 0
    for p in paths:
        if await _send_file_if_nonempty(update, p):
            sent += 1
    return sent


# =========================
# Commands
# =========================
async def start(update: Update, ctx: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text(
        "–ü—Ä–∏–≤–µ—Ç! –Ø LLM-–±–æ—Ç –¥–ª—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤. –ß—Ç–æ –¥–µ–ª–∞–µ–º?",
        reply_markup=main_menu(),
    )


async def help_cmd(update: Update, ctx: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text(
        "–ö–æ–º–∞–Ω–¥—ã: /menu /help\n"
        "‚Äî –ø—Ä–∏—à–ª–∏—Ç–µ —Ç–µ–∫—Å—Ç: –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É—é –¥–æ–º–µ–Ω\n"
        "‚Äî –ø—Ä–∏—à–ª–∏—Ç–µ .xlsx –∏–ª–∏ .csv: —Å–æ–±–µ—Ä—É –¥–∞—Ç–∞—Å–µ—Ç (train/eval) –∏ –≤—ã–≥—Ä—É–∂—É —Ñ–∞–π–ª—ã",
    )


async def menu_cmd(update: Update, ctx: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("–ì–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é:", reply_markup=main_menu())


async def stats_cmd(update: Update, ctx: ContextTypes.DEFAULT_TYPE):
    """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É feedback –∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏"""
    stats = feedback_learner.get_feedback_stats()
    
    if stats["total_feedback"] == 0:
        await update.message.reply_text("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—É—Å—Ç–∞ - –ø–æ–∫–∞ –Ω–µ—Ç feedback –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π.")
        return
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
    report_lines = [
        f"üìä **–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏**",
        f"",
        f"üî¢ –í—Å–µ–≥–æ feedback: {stats['total_feedback']}",
        f"‚úèÔ∏è –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π: {stats['corrections']}",
        f"üìà –ü—Ä–æ—Ü–µ–Ω—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π: {stats['correction_rate']:.1%}",
        f"üÜï –ó–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 7 –¥–Ω–µ–π: {stats['recent_corrections']}",
        f""
    ]
    
    if stats["top_errors"]:
        report_lines.extend([
            f"üö® **–¢–æ–ø –æ—à–∏–±–æ–∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:**",
            ""
        ])
        for (predicted, corrected), count in list(stats["top_errors"].items())[:5]:
            report_lines.append(f"‚Ä¢ `{predicted}` ‚Üí `{corrected}` ({count} —Ä–∞–∑)")
        report_lines.append("")
    
    if stats["corrected_domains"]:
        report_lines.extend([
            f"üéØ **–ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ –¥–æ–º–µ–Ω—ã (–ø–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º):**",
            ""
        ])
        for domain, count in list(stats["corrected_domains"].items())[:5]:
            report_lines.append(f"‚Ä¢ `{domain}`: {count}")
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫—ç—à–∞
    cache_stats = llm_cache.get_stats()
    if cache_stats["total_entries"] > 0:
        report_lines.extend([
            f"",
            f"üíæ **–ö—ç—à LLM:**",
            f"‚Ä¢ –ó–∞–ø–∏—Å–µ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {cache_stats['classification_entries']}",
            f"‚Ä¢ –ó–∞–ø–∏—Å–µ–π –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏: {cache_stats['augmentation_entries']}",
            f"‚Ä¢ TTL: {cache_stats['ttl_hours']} —á–∞—Å–æ–≤"
        ])
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
    context_stats = context_manager.get_stats()
    if context_stats["total_users"] > 0:
        report_lines.extend([
            f"",
            f"üë• **–ö–æ–Ω—Ç–µ–∫—Å—Ç—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π:**",
            f"‚Ä¢ –í—Å–µ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {context_stats['total_users']}",
            f"‚Ä¢ –ê–∫—Ç–∏–≤–Ω—ã—Ö (24—á): {context_stats['active_users']}",
            f"‚Ä¢ –í—Å–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏–π: {context_stats['total_messages']}"
        ])
        
        if context_stats["top_domains"]:
            top_domains_text = ", ".join([f"`{domain}` ({count:.0f})" 
                                        for domain, count in list(context_stats["top_domains"].items())[:3]])
            report_lines.append(f"‚Ä¢ –¢–æ–ø –¥–æ–º–µ–Ω—ã: {top_domains_text}")
    
    report_text = "\n".join(report_lines)
    await update.message.reply_text(report_text, parse_mode="Markdown")


# =========================
# Text classify
# =========================
async def on_text(update: Update, ctx: ContextTypes.DEFAULT_TYPE):
    text = (update.message.text or "").strip()
    if not text:
        await update.message.reply_text("–ü—É—Å—Ç–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ. –í–≤–µ–¥–∏—Ç–µ —Ñ—Ä–∞–∑—É –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ /menu.")
        return

    user_id = str(update.effective_user.id)
    
    # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    user_context = context_manager.get_classification_context(user_id)
    preferred_domains = context_manager.get_preferred_domains(user_id)
    
    # –î–æ–ø–æ–ª–Ω—è–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –µ—Å–ª–∏ –µ—Å—Ç—å
    enhanced_system_prompt = LABELER_SYSTEM
    if user_context:
        enhanced_system_prompt = f"{LABELER_SYSTEM}\n\n{user_context}"
    
    try:
        data = classify_one(
            label_llm, 
            enhanced_system_prompt, 
            LABELER_FEWSHOT, 
            text,
            allowed_labels=preferred_domains if preferred_domains else None
        )
    except Exception as e:
        logger.exception("classify_one failed", exc_info=e)
        await update.message.reply_text(f"–û—à–∏–±–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {e!s}")
        return

    domain = data.get("domain_id") or "unknown"
    conf = data.get("confidence", 0.0)
    cands = data.get("top_candidates") or [[domain, conf]]
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è feedback
    ctx.user_data["last_classification"] = {
        "text": text,
        "predicted_domain": domain,
        "confidence": conf,
        "candidates": cands
    }
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–ø–æ–∫–∞ –±–µ–∑ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏)
    context_manager.update_context(user_id, text, domain, confidence=conf)

    await update.message.reply_text(
        f"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {domain} (‚âà{conf:.2f})\n–í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π:",
        reply_markup=top_candidates_buttons(cands),
    )


# =========================
# Documents (.xlsx / .csv)
# =========================
async def on_document(update: Update, ctx: ContextTypes.DEFAULT_TYPE):
    doc = update.message.document
    name = (doc.file_name or "").lower()

    # –¢–µ–ø–µ—Ä—å –ø—Ä–∏–Ω–∏–º–∞–µ–º .xlsx –ò .csv
    if not (name.endswith(".xlsx") or name.endswith(".csv")):
        await update.message.reply_text(
            "–ü—Ä–∏—à–ª–∏—Ç–µ .xlsx –∏–ª–∏ .csv —Å –ª–æ–≥–∞–º–∏. –ú–∏–Ω–∏–º—É–º ‚Äî –∫–æ–ª–æ–Ω–∫–∞ —Ç–µ–∫—Å—Ç–∞ (`text` / `query_text`)."
        )
        return

    # –°–∫–∞—á–∏–≤–∞–µ–º —Ñ–∞–π–ª
    tg_file = await doc.get_file()
    buf = await tg_file.download_as_bytearray()
    upath = Path(settings.data_dir) / f"logs_{update.effective_user.id}{Path(name).suffix}"
    with open(upath, "wb") as f:
        f.write(buf)

    # ETL ‚Üí DataFrame (—É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä CSV/XLSX)
    await update.message.reply_text("–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Ñ–∞–π–ª‚Ä¶")
    try:
        df = normalize_file_to_df(upath, max_rows=getattr(settings, "max_batch", None))
        save_parquet_or_csv(df, base_dir=Path(settings.data_dir))
    except Exception as e:
        logger.exception("ETL failed", exc_info=e)
        await update.message.reply_text(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {e!s}")
        return

    if df.empty:
        await update.message.reply_text(
            "–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±–Ω–∞—Ä—É–∂–∏—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Å—Ç—Ä–æ–∫–∏.\n"
            "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ–±—ã –∫–æ–ª–æ–Ω–∫–∞ —Å –∑–∞–ø—Ä–æ—Å–∞–º–∏ –±—ã–ª–∞ –Ω–∞–∑–≤–∞–Ω–∞ `text` –∏–ª–∏ `query_text` (–∏–ª–∏ —Å–∏–Ω–æ–Ω–∏–º –∏–∑ README)."
        )
        return

    # Batched LLM labeling —Å –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º
    progress = ProgressTracker(update, ctx, len(df), "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤")
    
    try:
        rows = await label_dataframe_batched_with_progress(
            df,
            label_llm,
            LABELER_SYSTEM,
            LABELER_FEWSHOT,
            progress,
            getattr(settings, "batch_size", 20),
            getattr(settings, "rate_limit", 0.4),
        )
    except Exception as e:
        logger.exception("label_dataframe_batched failed", exc_info=e)
        await progress.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–∑–º–µ—Ç–∫–µ: {e!s}")
        return

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º CSV —Å —Ä–∞–∑–º–µ—Ç–∫–æ–π
    try:
        labeled_csv = store.save_labeled_csv(rows)
    except Exception as e:
        logger.exception("save_labeled_csv failed", exc_info=e)
        await update.message.reply_text(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ CSV: {e!s}")
        return

    # HITL –æ—á–µ—Ä–µ–¥—å (–Ω–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å)
    low = []
    try:
        low = low_conf_items(rows, getattr(settings, "low_conf", 0.5))
        store.append_hitl_queue(low)
    except Exception as e:
        logger.exception("HITL queue append failed", exc_info=e)

    # –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è
    try:
        aug_records = await augment_dataset(
            aug_llm,
            AUG_SYSTEM,
            rows,
            rate_limit=getattr(settings, "rate_limit", 0.4),
            include_low_conf=getattr(settings, "augment_include_lowconf", False),
            low_conf_threshold=getattr(settings, "low_conf", 0.5),
        )
    except Exception as e:
        logger.exception("augment_dataset failed", exc_info=e)
        await update.message.reply_text(f"–û—à–∏–±–∫–∞ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏: {e!s}")
        return

    # –ï—Å–ª–∏ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –Ω–∏—á–µ–≥–æ –Ω–µ –¥–∞–ª–∞ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–µ
    base_records = aug_records if aug_records else rows

    # –°–ø–ª–∏—Ç train/eval
    train, eval_ = split_train_eval(base_records, eval_frac=0.10, min_eval=50)

    # –ü–∏—à–µ–º JSONL (–±–µ–∑ 0-–±–∞–π—Ç–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤)
    art = store.path("artifacts")
    train_p = art / "dataset_train.jsonl"
    eval_p  = art / "dataset_eval.jsonl"
    ok_train = write_jsonl(train_p, train)
    ok_eval  = write_jsonl(eval_p,  eval_)

    # –†–µ–∑—é–º–µ –∏ –æ—Ç–ø—Ä–∞–≤–∫–∞
    msg = (
        f"–ì–æ—Ç–æ–≤–æ. –†–∞–∑–º–µ—á–µ–Ω–æ: {len(rows)}; –Ω–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {len(low)}\n"
        f"train={len(train)}, eval={len(eval_)}\n"
        f"–û—Ç–ø—Ä–∞–≤–ª—è—é —Ñ–∞–π–ª—ã‚Ä¶"
    )
    await update.message.reply_text(msg)

    to_send = [labeled_csv]
    if ok_train: to_send.append(train_p)
    else: await update.message.reply_text("‚ö†Ô∏è –§–∞–π–ª dataset_train.jsonl –Ω–µ —Å–æ–∑–¥–∞–Ω ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞—é.")
    if ok_eval: to_send.append(eval_p)
    else: await update.message.reply_text("‚ö†Ô∏è –§–∞–π–ª dataset_eval.jsonl –Ω–µ —Å–æ–∑–¥–∞–Ω ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞—é.")
    await _send_files_batch(update, to_send)


# =========================
# Callbacks (inline buttons)
# =========================
async def on_cb(update: Update, ctx: ContextTypes.DEFAULT_TYPE):
    q = update.callback_query
    data = q.data or ""
    await q.answer()

    if data == "menu_classify":
        await q.edit_message_text("–ù–∞–ø–∏—à–∏—Ç–µ —Ñ—Ä–∞–∑—É –æ–¥–Ω–∏–º —Å–æ–æ–±—â–µ–Ω–∏–µ–º ‚Äî —è –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É—é –¥–æ–º–µ–Ω.")
        return

    if data == "menu_upload":
        await q.edit_message_text("–û—Ç–ø—Ä–∞–≤—å—Ç–µ .xlsx –∏–ª–∏ .csv —Ñ–∞–π–ª —Å –ª–æ–≥–∞–º–∏ (–∫–æ–ª–æ–Ω–∫–∞ `text` –∏–ª–∏ `query_text`).")
        return

    if data == "menu_export":
        tp = store.path("artifacts/dataset_train.jsonl")
        ep = store.path("artifacts/dataset_eval.jsonl")
        if (not tp.exists() or tp.stat().st_size == 0) and (not ep.exists() or ep.stat().st_size == 0):
            await q.edit_message_text("–î–∞—Ç–∞—Å–µ—Ç—ã –µ—â—ë –Ω–µ –≥–æ—Ç–æ–≤—ã. –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª –∏ –¥–æ–∂–¥–∏—Ç–µ—Å—å –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
            return
        await q.edit_message_text("–û—Ç–ø—Ä–∞–≤–ª—è—é –¥–∞—Ç–∞—Å–µ—Ç—ã‚Ä¶")
        chat_id = q.message.chat_id
        for p in [tp, ep]:
            try:
                if not p.exists():
                    await ctx.bot.send_message(chat_id, f"‚ö†Ô∏è –§–∞–π–ª {p.name} –Ω–µ –Ω–∞–π–¥–µ–Ω ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞—é.")
                    continue
                if p.stat().st_size == 0:
                    await ctx.bot.send_message(chat_id, f"‚ö†Ô∏è –§–∞–π–ª {p.name} –ø—É—Å—Ç ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞—é.")
                    continue
                with open(p, "rb") as f:
                    await ctx.bot.send_document(chat_id=chat_id, document=InputFile(f, filename=p.name))
            except Exception as e:
                await ctx.bot.send_message(chat_id, f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å {p.name}: {e!s}")
        return

    if data == "menu_hitl":
        items = store.read_hitl_queue(limit=10)
        if not items:
            await q.edit_message_text("–û—á–µ—Ä–µ–¥—å —Ä–µ–≤—å—é –ø—É—Å—Ç–∞. üëç")
            return
        item = items[0]
        text = item.get("text", "")
        cands = item.get("top_candidates", [])
        if isinstance(cands, str):
            try:
                cands = json.loads(cands)
            except Exception:
                cands = []
        doms = [d for d, _ in cands] or ["payments", "mfc", "housing", "transport", "health", "oos"]
        await q.edit_message_text(
            f"HITL #1/‚Ä¶\n–¢–µ–∫—Å—Ç: {text}\n–í—ã–±–µ—Ä–∏—Ç–µ –¥–æ–º–µ–Ω:",
            reply_markup=hitl_item_buttons(doms),
        )
        ctx.user_data["hitl_current"] = item
        return

    if data.startswith("pick_domain:"):
        dom = data.split(":", 1)[1]
        
        # –õ–æ–≥–∏—Ä—É–µ–º feedback –µ—Å–ª–∏ –µ—Å—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ–¥–Ω–µ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        if "last_classification" in ctx.user_data:
            last_class = ctx.user_data["last_classification"]
            user_id = str(q.from_user.id)
            
            # –õ–æ–≥–∏—Ä—É–µ–º –≤ —Å–∏—Å—Ç–µ–º—É –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
            feedback_learner.log_feedback(
                text=last_class["text"],
                predicted_domain=last_class["predicted_domain"],
                corrected_domain=dom,
                confidence=last_class["confidence"],
                user_id=user_id
            )
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º
            context_manager.update_context(
                user_id, 
                last_class["text"], 
                last_class["predicted_domain"],
                corrected_domain=dom,
                confidence=last_class["confidence"]
            )
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–º–ø—Ç—ã –µ—Å–ª–∏ –Ω–∞–∫–æ–ø–∏–ª–æ—Å—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ feedback
            if prompt_optimizer.should_retrain():
                global LABELER_SYSTEM, LABELER_FEWSHOT
                LABELER_SYSTEM, LABELER_FEWSHOT = get_optimized_prompts()
                logger.info("–ü—Ä–æ–º–ø—Ç—ã –æ–±–Ω–æ–≤–ª–µ–Ω—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ feedback")
        
        await q.edit_message_text(f"–°–ø–∞—Å–∏–±–æ! –ó–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–ª: {dom}. –ú–æ–∂–Ω–æ –ø—Ä–∏—Å—ã–ª–∞—Ç—å –Ω–æ–≤—É—é —Ñ—Ä–∞–∑—É –∏–ª–∏ /menu.")
        return

    if data.startswith("hitl_choose:"):
        dom = data.split(":", 1)[1]
        item = ctx.user_data.get("hitl_current")
        if item:
            item["domain_true"] = dom
            store.append_hitl_queue([item])  # MVP: —Å–∫–ª–∞–¥—ã–≤–∞–µ–º –≤–µ—Ä–¥–∏–∫—Ç —Ç—É–¥–∞ –∂–µ
        await q.edit_message_text(f"–ü–æ–º–µ—Ç–∏–ª –∫–∞–∫: {dom}. /menu –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è.")
        return

    if data == "hitl_skip":
        await q.edit_message_text("–ü—Ä–æ–ø—É—â–µ–Ω–æ. /menu")
        return


# =========================
# Error handler
# =========================
async def error_handler(update: object, ctx: ContextTypes.DEFAULT_TYPE):
    logger.exception("Unhandled error", exc_info=ctx.error)


# =========================
# Wiring
# =========================
application.add_handler(CommandHandler("start", start))
application.add_handler(CommandHandler("help", help_cmd))
application.add_handler(CommandHandler("menu", menu_cmd))
application.add_handler(CommandHandler("stats", stats_cmd))
application.add_handler(CallbackQueryHandler(on_cb))

# –ø–æ—Ä—è–¥–æ–∫ –≤–∞–∂–µ–Ω: —Å–Ω–∞—á–∞–ª–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã, –∑–∞—Ç–µ–º —Ç–µ–∫—Å—Ç (—á—Ç–æ–±—ã –ø–æ–¥–ø–∏—Å–∏ –∫ —Ñ–∞–π–ª–∞–º –Ω–µ –ª–æ–≤–∏–ª–∏—Å—å –∫–∞–∫ —Ç–µ–∫—Å—Ç)
application.add_handler(MessageHandler(filters.Document.ALL, on_document))
application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, on_text))

application.add_error_handler(error_handler)

if __name__ == "__main__":
    if settings.public_url:
        logging.info("WEBHOOK mode")
        application.run_webhook(
            listen="0.0.0.0",
            port=settings.port,
            url_path=settings.bot_token,
            webhook_url=f"{settings.public_url}/{settings.bot_token}",
            drop_pending_updates=True,
        )
    else:
        logging.info("POLLING mode")
        application.run_polling(
            drop_pending_updates=True,
            allowed_updates=["message", "callback_query", "inline_query", "chat_member"],
        )
